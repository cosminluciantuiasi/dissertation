{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd7U0vR-4I0o",
        "outputId": "23a07e7f-b591-442f-815b-a2cd4bc69069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.3.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.2/404.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.11.2-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/225.3 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.10.0 (from optuna)\n",
            "  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.19)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.7.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.2 cmaes-0.10.0 colorlog-6.7.0 optuna-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Umu9-vf4XXg",
        "outputId": "e02a5f87-e4a3-4ea0-ce6b-e595765c20de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna-dashboard\n",
            "  Downloading optuna_dashboard-0.12.0-py3-none-any.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bottle (from optuna-dashboard)\n",
            "  Downloading bottle-0.12.25-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.2/90.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: optuna>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from optuna-dashboard) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optuna-dashboard) (23.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from optuna-dashboard) (1.2.2)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=2.4.0->optuna-dashboard) (1.11.2)\n",
            "Requirement already satisfied: cmaes>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=2.4.0->optuna-dashboard) (0.10.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna>=2.4.0->optuna-dashboard) (6.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna>=2.4.0->optuna-dashboard) (1.23.5)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=2.4.0->optuna-dashboard) (2.0.19)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna>=2.4.0->optuna-dashboard) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna>=2.4.0->optuna-dashboard) (6.0.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->optuna-dashboard) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->optuna-dashboard) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->optuna-dashboard) (3.2.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna>=2.4.0->optuna-dashboard) (1.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna>=2.4.0->optuna-dashboard) (4.7.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=2.4.0->optuna-dashboard) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna>=2.4.0->optuna-dashboard) (2.1.3)\n",
            "Installing collected packages: bottle, optuna-dashboard\n",
            "Successfully installed bottle-0.12.25 optuna-dashboard-0.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna-dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TwnUAvRRru3t"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import fnmatch\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "import glob\n",
        "import random\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.transforms.functional import to_tensor, normalize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import Compose, ToTensor, RandomHorizontalFlip, RandomResizedCrop, ColorJitter, Grayscale\n",
        "\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "from optuna.storages import RetryFailedTrialCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26MwJpu0oWKw"
      },
      "outputs": [],
      "source": [
        "# !optuna-dashboard sqlite:///db.sqlite3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwmT0IdImlmd",
        "outputId": "2e6993ba-8f5c-4a99-a0d5-c96e45d3b701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5oeNy_Xf32Zj"
      },
      "outputs": [],
      "source": [
        "optuna_db_path = \"/content/drive/MyDrive/Colab Notebooks/dissertation/optuna_db_sqlite/optuna_data.db\"\n",
        "checkpoints_path = \"/content/drive/MyDrive/checkpoints\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41D0MfeYKqTF",
        "outputId": "738c8f2b-f7fe-43d0-d82f-8b33459a4e46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yaleB11  yaleB16  yaleB20  yaleB24  yaleB28  yaleB32  yaleB36\n",
            "yaleB12  yaleB17  yaleB21  yaleB25  yaleB29  yaleB33  yaleB37\n",
            "yaleB13  yaleB18  yaleB22  yaleB26  yaleB30  yaleB34  yaleB38\n",
            "yaleB15  yaleB19  yaleB23  yaleB27  yaleB31  yaleB35  yaleB39\n"
          ]
        }
      ],
      "source": [
        "!ls '/content/drive/MyDrive/Colab Notebooks/dissertation/transformed_yale_extended2'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find '/content/drive/MyDrive/Colab Notebooks/dissertation/transformed_yale_extended2' | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RHD685aOc-E",
        "outputId": "448c8c81-3c9b-4808-9a41-1099cccdd7f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ajcGeMxDmSK"
      },
      "source": [
        "**FYI**: The following lines of code unzip the original dataset and apply some basic preprocessing operations (haar cascade classifier to crop faces). At the moment I use direclty the preprocessed dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYFvYaWYpNYK"
      },
      "outputs": [],
      "source": [
        "# !unzip '/content/drive/MyDrive/Colab Notebooks/disertation/dataset/ExtendedYaleB.zip' -d /tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q14iPZoK2V8V"
      },
      "outputs": [],
      "source": [
        "# # Load the cascade\n",
        "# face_cascade = cv2.CascadeClassifier('/content/drive/MyDrive/Colab Notebooks/disertation/misc/haar.xml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR5ddhzzq0Sx"
      },
      "outputs": [],
      "source": [
        "# extensions = ['*.jpg', '*.png', '*.jpeg', '*.pgm']\n",
        "# #CV2 Cascade Classifier hyperparams\n",
        "# scaleFactor = 1.1\n",
        "# minNeighbors = 4\n",
        "\n",
        "# def detect_faces_in_dataset(input_path, output_path):\n",
        "#   if not os.path.exists(input_path) or not os.path.exists(output_path):\n",
        "#     print('Provided file paths dont exist.')\n",
        "#     return\n",
        "#   for root,_,files in os.walk(input_path):\n",
        "#     print(root)\n",
        "#     if root==input_path:\n",
        "#       continue\n",
        "\n",
        "#     subclass = root.split('/')[-1]\n",
        "#     subclass_path = output_path+'/'+subclass\n",
        "\n",
        "#     if not os.path.exists(subclass_path):\n",
        "#       os.mkdir(subclass_path)\n",
        "\n",
        "#     for filename in files:\n",
        "#       print(filename)\n",
        "#       file = os.path.join(root,filename)\n",
        "#       counter=0\n",
        "#       if any(fnmatch.fnmatch(file, extension) for extension in extensions):\n",
        "#         img = cv2.imread(file)\n",
        "#         gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "#         # Detect faces coords\n",
        "#         faces = face_cascade.detectMultiScale(gray_img, scaleFactor = scaleFactor, minNeighbors = minNeighbors)\n",
        "#         # Draw rectangle around the faces\n",
        "#         for (x, y, w, h) in faces:\n",
        "#           crop_face = img[y:y+h, x:x+w]\n",
        "#           new_filename = '.'.join([filename.split('.')[0] + \"_\" + str(counter),'png'])\n",
        "\n",
        "#           file_path = '/'.join(root.split('/')[:-1])\n",
        "#           file_path = os.path.join(output_path+'/'+subclass,new_filename)\n",
        "#           print(file_path)\n",
        "#           cv2.imwrite(file_path,crop_face)\n",
        "#           print(f\"Saved {file_path}.\")\n",
        "\n",
        "#           counter+=1\n",
        "#           break\n",
        "#         if len(faces)==0:\n",
        "#           print(f'No face detected on {filename}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8S4lnHnMTtJ"
      },
      "outputs": [],
      "source": [
        "# input_path = \"/content/drive/MyDrive/Colab Notebooks/disertation/dataset/test_images\"\n",
        "# output_path = \"/content/drive/MyDrive/Colab Notebooks/disertation/dataset/transformed_images\"\n",
        "# detect_faces_in_dataset(input_path, output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPgscBRZXUWC"
      },
      "outputs": [],
      "source": [
        "# !mkdir '/content/drive/MyDrive/Colab Notebooks/disertation/transformed_yale_extended2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SvrEKblkopQ"
      },
      "outputs": [],
      "source": [
        "# # input_path = \"/tmp/PINS\"\n",
        "# # output_path = \"/tmp/transformed_pins\"\n",
        "\n",
        "\n",
        "# input_path = \"/tmp/ExtendedYaleB\"\n",
        "# output_path = \"/content/drive/MyDrive/Colab Notebooks/disertation/transformed_yale_extended2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE6qn_rLP7lb"
      },
      "outputs": [],
      "source": [
        "# detect_faces_in_dataset(input_path, output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcvOLB7VQrk7"
      },
      "outputs": [],
      "source": [
        "# !rm -r /tmp/transformed_pins/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GKyhjDsMYjvc"
      },
      "outputs": [],
      "source": [
        "class FacesDataset(Dataset):\n",
        "    def __init__(self, file_list, dataset_path, width=128, height=128, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "        self.img_size = (width, height)\n",
        "        self.dataset_path = dataset_path\n",
        "         # Default transform without augmentation\n",
        "        self.default_transform = Compose([\n",
        "            ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        img = Image.open(img_path).convert(\"L\")\n",
        "\n",
        "        original_width, original_height = img.size\n",
        "\n",
        "        img = img.resize(self.img_size)\n",
        "        label = img_path.split('/')\n",
        "        label_name = label[-2]\n",
        "\n",
        "        labels = os.listdir(self.dataset_path)\n",
        "        labels.sort()\n",
        "        label = labels.index(label[-2])\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        else:\n",
        "            img = self.default_transform(img)\n",
        "\n",
        "        return img, (label,label_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ox-UxKx9ZiuQ"
      },
      "outputs": [],
      "source": [
        "def collate_fn(examples):\n",
        "  processed_images = []\n",
        "  processed_labels = []\n",
        "\n",
        "  for example in examples:\n",
        "    tensor_image = example[0]\n",
        "    normalized_tensor_image = normalize(tensor_image, [0.5], [0.5])\n",
        "    normalized_tensor_image = normalized_tensor_image.unsqueeze(0)\n",
        "    processed_images.append(normalized_tensor_image)\n",
        "    label = np.array(example[1][0])\n",
        "\n",
        "    tensor_label = torch.tensor(label)\n",
        "    tensor_label = tensor_label.unsqueeze(0)\n",
        "    processed_labels.append(tensor_label)\n",
        "\n",
        "  torch_images = torch.cat(processed_images, dim=0)\n",
        "  torch_labels = torch.cat(processed_labels, dim=0)\n",
        "  return torch_images, torch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cncENkjOSql3"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset_path=\"/content/drive/MyDrive/Colab Notebooks/dissertation/transformed_yale_extended2\", train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, num_classes=28): #dataset_path\n",
        "  train_transform = Compose([\n",
        "      RandomHorizontalFlip(p=0.5),                           # Random horizontal flip\n",
        "      ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),  # Random color jitter\n",
        "      ToTensor(),                                            # Convert the image to a tensor\n",
        "  ])\n",
        "\n",
        "  file_list = glob.glob(os.path.join(dataset_path+'/*','*.png'))\n",
        "  random.shuffle(file_list)\n",
        "\n",
        "  dataset = FacesDataset(file_list, dataset_path, transform=None)\n",
        "\n",
        "  dataset_size = len(dataset)\n",
        "  train_size = int(train_ratio * dataset_size)\n",
        "  val_size = int(val_ratio * dataset_size)\n",
        "  test_size = dataset_size - train_size - val_size\n",
        "\n",
        "  print(f\"Total dataset size: {dataset_size}; split in train={train_size}, validation={val_size}, test={test_size}\")\n",
        "\n",
        "  train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "  return train_dataset, val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAXHRegFYFO_"
      },
      "outputs": [],
      "source": [
        "# see_examples = 2\n",
        "# for i, (imgs, label_id) in enumerate(train_loader):\n",
        "#   clear_output(wait=True)\n",
        "#   plt.imshow(imgs[0].permute(1, 2, 0)[:,:,0],cmap='gray')\n",
        "#   plot_text = f'id: {str(label_id[0].item())}'\n",
        "#   plt.text(5, 5, plot_text, fontsize ='xx-large', color='red', fontweight='bold')\n",
        "#   plt.show()\n",
        "\n",
        "#   if i >= see_examples - 1:\n",
        "#     break\n",
        "#   time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbKkI5OtEJII"
      },
      "source": [
        "Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mxkSWXRJgu0-"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self , in_channels , out_channels , kernel_size , stride , padding, mp_kernel_size, mp_stride, dropout_rate):\n",
        "    super(ConvBlock , self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels , out_channels , kernel_size , stride, padding)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=dropout_rate)\n",
        "    self.max_pool2d = nn.MaxPool2d(kernel_size = mp_kernel_size, stride = mp_stride)\n",
        "\n",
        "  def forward(self , x):\n",
        "    out = self.conv(x)\n",
        "    out = self.activation(out)\n",
        "    out = self.dropout(out)\n",
        "    out = self.max_pool2d(out)\n",
        "    return out\n",
        "\n",
        "class FacesModel(nn.Module):\n",
        "  def __init__(\n",
        "        self,\n",
        "        nr_conv_blocks,\n",
        "        in_channels_list,\n",
        "        out_channels_list,\n",
        "        kernel_size_list,\n",
        "        stride_list,\n",
        "        padding_list,\n",
        "        mp_kernel_size_list,\n",
        "        mp_stride_list,\n",
        "        dropout_rate_list,\n",
        "        nr_fc_layers,\n",
        "        fc_sizes_list,\n",
        "        num_classes=28,\n",
        "  ):\n",
        "    super(FacesModel, self).__init__()\n",
        "\n",
        "    h, w = 128, 128\n",
        "    #Conv layers\n",
        "    conv_blocks = []\n",
        "    for i in range(nr_conv_blocks):\n",
        "      conv_blocks.append(\n",
        "          ConvBlock(\n",
        "            in_channels=in_channels_list[i],\n",
        "            out_channels=out_channels_list[i],\n",
        "            kernel_size=kernel_size_list[i],\n",
        "            stride=stride_list[i],\n",
        "            padding=padding_list[i],\n",
        "            mp_kernel_size=mp_kernel_size_list[i],\n",
        "            mp_stride=mp_stride_list[i],\n",
        "            dropout_rate=dropout_rate_list[i],\n",
        "          )\n",
        "      )\n",
        "      h = ((h - kernel_size_list[i] + 2 * padding_list[i]) // stride_list[i]) + 1\n",
        "      h = ((h - mp_kernel_size_list[i]) // mp_stride_list[i]) + 1\n",
        "      w = ((w - kernel_size_list[i] + 2 * padding_list[i]) // stride_list[i]) + 1\n",
        "      w = ((w - mp_kernel_size_list[i]) // mp_stride_list[i]) + 1\n",
        "    self.conv_seq = nn.Sequential(*conv_blocks)\n",
        "\n",
        "    #FC layers\n",
        "    fc_layers = []\n",
        "    in_features = out_channels_list[-1] * h * w\n",
        "    # print(in_features)\n",
        "    for i in range(nr_fc_layers):\n",
        "      out_features = fc_sizes_list[i]\n",
        "      fc_layers.append(nn.Linear(in_features, out_features))\n",
        "      fc_layers.append(nn.ReLU())\n",
        "      fc_layers.append(nn.Dropout(p=dropout_rate_list[nr_conv_blocks + i]))\n",
        "      in_features = out_features\n",
        "    fc_layers.append(nn.Linear(in_features, num_classes))\n",
        "    self.fc_seq = nn.Sequential(*fc_layers)\n",
        "\n",
        "  def forward(self , x):\n",
        "    out = self.conv_seq(x)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.fc_seq(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EOuTC8cFtSTt"
      },
      "outputs": [],
      "source": [
        "def val_loop(val_loader, model, loss_fn, device):\n",
        "  #set model to val mode\n",
        "  model.eval()\n",
        "  size = len(val_loader.dataset)\n",
        "  num_batches = len(val_loader)\n",
        "  val_loss, correct = 0, 0\n",
        "\n",
        "  #make sure no gradient is computed in validation phase\n",
        "  with torch.no_grad():\n",
        "    for val_images, val_labels in val_loader:\n",
        "      val_images = val_images.to(device)\n",
        "      val_labels = val_labels.to(device)\n",
        "      pred = model(val_images)\n",
        "      val_loss += loss_fn(pred, val_labels).item()\n",
        "      correct += torch.sum(torch.argmax(pred,dim=1) == val_labels)\n",
        "  val_loss /= num_batches\n",
        "  correct = float(correct)/ float(size)\n",
        "  print(f\"Validation: \\n Val - Accuracy: {(100*correct):>0.1f}%, Val - loss: {val_loss:>8f};\\n\")\n",
        "  return correct, val_loss\n",
        "\n",
        "def train_loop(train_loader, model, optimizer, loss_fn, device):\n",
        "  size = len(train_loader.dataset)\n",
        "  running_loss, last_loss = 0.0, 0.0\n",
        "  report_step = 50\n",
        "\n",
        "  #set model to train mode - useful for batch norm or dropout\n",
        "  model.train()\n",
        "  for counter, (train_images, train_labels) in enumerate(train_loader):\n",
        "    train_images = train_images.to(device)\n",
        "    train_labels = train_labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(train_images)\n",
        "    train_loss = loss_fn(pred, train_labels)\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += train_loss.item()\n",
        "    if counter % report_step == 0:\n",
        "      last_loss = running_loss / report_step if counter !=0 else running_loss\n",
        "      processed_images =  (counter+1) * len(train_images)\n",
        "      print(f\"Training loss: {last_loss:>7f}  [{processed_images:>5d}/{size:>5d}]\")\n",
        "      running_loss = 0.0\n",
        "  return last_loss\n",
        "\n",
        "\n",
        "def test_model(model, device):\n",
        "  _, _, test_dataset = load_data()\n",
        "  test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=1, collate_fn=collate_fn)\n",
        "\n",
        "  total = 0\n",
        "  correct = 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i, (x, y) in enumerate(test_loader):\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      pred = model(x)\n",
        "      correct += torch.sum(pred.argmax(dim=1) == y)\n",
        "      total += len(x)\n",
        "\n",
        "      if i % 50 == 0:\n",
        "        print(f\"Correct: {int(correct)}, Total checked: {total}\")\n",
        "\n",
        "  accuracy = correct / total\n",
        "  print(f\"Accuracy: {correct} / {total} = {accuracy:.2%}\")\n",
        "\n",
        "  return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "f_MrDum29x9C"
      },
      "outputs": [],
      "source": [
        "def objective(trial, device):\n",
        "  print(f\"\\n\\n{'#' * 45}\\n#{' ' * 15}Starting trial - {trial.number}{' ' * 15}#\\n{'#' * 45}\\n\")\n",
        "  # Define the hyperparameter search space using Optuna suggest methods\n",
        "  nr_conv_blocks = trial.suggest_int(\"nr_conv_blocks\", 2, 5)\n",
        "  out_channels_list = [trial.suggest_int(f\"out_channels_{i}\", 32, 128) for i in range(nr_conv_blocks)]\n",
        "  in_channels_list = [1] + out_channels_list[:-1]\n",
        "  kernel_size_list = [3] * nr_conv_blocks\n",
        "  stride_list = [1] * nr_conv_blocks\n",
        "  padding_list = [1] * nr_conv_blocks\n",
        "  mp_kernel_size_list = [2] * nr_conv_blocks\n",
        "  mp_stride_list = [2] * nr_conv_blocks\n",
        "  dropout_rate_list = [trial.suggest_float(f\"dropout_rate_{i}\", 0, 0.7) for i in range(nr_conv_blocks)]\n",
        "\n",
        "  nr_fc_layers = trial.suggest_int(\"nr_fc_layers\", 1, 4)\n",
        "  fc_sizes_list = [trial.suggest_int(f\"fc_size_{i}\", 128, 2048) for i in range(nr_fc_layers)]\n",
        "  dropout_rate_list += [trial.suggest_float(f\"dropout_rate_fc_{i}\", 0.0, 0.7) for i in range(nr_fc_layers)]\n",
        "\n",
        "  # Create the model with the suggested hyperparameters\n",
        "  model = FacesModel(\n",
        "      nr_conv_blocks=nr_conv_blocks,\n",
        "      in_channels_list=in_channels_list,\n",
        "      out_channels_list=out_channels_list,\n",
        "      kernel_size_list=kernel_size_list,\n",
        "      stride_list=stride_list,\n",
        "      padding_list=padding_list,\n",
        "      mp_kernel_size_list=mp_kernel_size_list,\n",
        "      mp_stride_list=mp_stride_list,\n",
        "      dropout_rate_list=dropout_rate_list,\n",
        "      nr_fc_layers=nr_fc_layers,\n",
        "      fc_sizes_list=fc_sizes_list,\n",
        "  ).to(device)\n",
        "\n",
        "  print(\"Model\")\n",
        "  print(model)\n",
        "  print(\"Model Summary\")\n",
        "  print(summary(model,(1,128,128)))\n",
        "\n",
        "  optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "  lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "  optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "  epochs = 30\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256])\n",
        "  train_dataset, val_dataset, test_dataset = load_data()\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size,num_workers=2, collate_fn=collate_fn)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2, collate_fn=collate_fn)\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(f\"\\n{'-'*42} Epoch={e+1} {'-'*42}\\n\")\n",
        "    train_loss = train_loop(train_loader, model, optimizer, loss_fn, device)\n",
        "    val_accuracy, val_loss = val_loop(val_loader, model, loss_fn, device)\n",
        "\n",
        "    trial.report(val_accuracy, e)\n",
        "    # For pruning (stops trial early if not promising)\n",
        "    if trial.should_prune():\n",
        "      raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "  print(f\"{'#' * 45}\\n#{' ' * 15}Finished trial - {trial.number}/{number_of_trials}{' ' * 15}#\\n{'#' * 45}\")\n",
        "  return val_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WkYdnIEhdTsp"
      },
      "outputs": [],
      "source": [
        "def execute():\n",
        "  storage = optuna.storages.RDBStorage(\n",
        "    f\"sqlite:///{optuna_db_path}\",\n",
        "    heartbeat_interval=1,\n",
        "    failed_trial_callback=RetryFailedTrialCallback()\n",
        "  )\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  global number_of_trials\n",
        "  number_of_trials = 30\n",
        "\n",
        "  global study\n",
        "  study = optuna.create_study(\n",
        "      storage=storage,\n",
        "      study_name=\"pytorch_faces\",\n",
        "      direction=\"maximize\",\n",
        "      load_if_exists=True\n",
        "  )\n",
        "\n",
        "  #https://optuna.readthedocs.io/en/latest/faq.html#how-to-define-objective-functions-that-have-own-arguments\n",
        "  study.optimize(lambda trial: objective(trial, device), n_trials=number_of_trials, timeout=None)\n",
        "\n",
        "  # -------------------------------------------------------------------------\n",
        "  # Results\n",
        "  # -------------------------------------------------------------------------\n",
        "\n",
        "  # Find number of pruned and completed trials\n",
        "  pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "  complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "\n",
        "  print(\"\\n\\nStudy statistics: \")\n",
        "  print(\"  Number of finished trials: \", len(study.trials))\n",
        "  print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "  print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "  print(\"Best trial:\")\n",
        "  trial = study.best_trial\n",
        "\n",
        "  print(\"  Value: \", trial.value)\n",
        "\n",
        "  print(\"  Params: \")\n",
        "  for key, value in trial.params.items():\n",
        "      print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "  # The line of the resumed trial's intermediate values begins with the restarted epoch.\n",
        "  optuna.visualization.plot_intermediate_values(study).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "m9wGvJbqsudd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e88f585c-77be-48b8-9268-172975bfa95d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-6f72f23c990c>:5: ExperimentalWarning: RetryFailedTrialCallback is experimental (supported from v2.8.0). The interface can change in the future.\n",
            "  failed_trial_callback=RetryFailedTrialCallback()\n",
            "[I 2023-08-16 08:15:42,154] A new study created in RDB with name: pytorch_faces\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:189: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 0               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 103, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.09311226923965477, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(103, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5274190264337794, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(108, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5318355104524414, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=14336, out_features=1061, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.026787442130207516, inplace=False)\n",
            "    (3): Linear(in_features=1061, out_features=1296, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.21988561645437563, inplace=False)\n",
            "    (6): Linear(in_features=1296, out_features=282, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.531925035703286, inplace=False)\n",
            "    (9): Linear(in_features=282, out_features=335, bias=True)\n",
            "    (10): ReLU()\n",
            "    (11): Dropout(p=0.08818594703494743, inplace=False)\n",
            "    (12): Linear(in_features=335, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 103, 128, 128]           1,030\n",
            "              ReLU-2        [-1, 103, 128, 128]               0\n",
            "           Dropout-3        [-1, 103, 128, 128]               0\n",
            "         MaxPool2d-4          [-1, 103, 64, 64]               0\n",
            "         ConvBlock-5          [-1, 103, 64, 64]               0\n",
            "            Conv2d-6          [-1, 108, 64, 64]         100,224\n",
            "              ReLU-7          [-1, 108, 64, 64]               0\n",
            "           Dropout-8          [-1, 108, 64, 64]               0\n",
            "         MaxPool2d-9          [-1, 108, 32, 32]               0\n",
            "        ConvBlock-10          [-1, 108, 32, 32]               0\n",
            "           Conv2d-11           [-1, 56, 32, 32]          54,488\n",
            "             ReLU-12           [-1, 56, 32, 32]               0\n",
            "          Dropout-13           [-1, 56, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 56, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 56, 16, 16]               0\n",
            "           Linear-16                 [-1, 1061]      15,211,557\n",
            "             ReLU-17                 [-1, 1061]               0\n",
            "          Dropout-18                 [-1, 1061]               0\n",
            "           Linear-19                 [-1, 1296]       1,376,352\n",
            "             ReLU-20                 [-1, 1296]               0\n",
            "          Dropout-21                 [-1, 1296]               0\n",
            "           Linear-22                  [-1, 282]         365,754\n",
            "             ReLU-23                  [-1, 282]               0\n",
            "          Dropout-24                  [-1, 282]               0\n",
            "           Linear-25                  [-1, 335]          94,805\n",
            "             ReLU-26                  [-1, 335]               0\n",
            "          Dropout-27                  [-1, 335]               0\n",
            "           Linear-28                   [-1, 28]           9,408\n",
            "================================================================\n",
            "Total params: 17,213,618\n",
            "Trainable params: 17,213,618\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 58.47\n",
            "Params size (MB): 65.66\n",
            "Estimated Total Size (MB): 124.20\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.333375  [  128/ 8582]\n",
            "Training loss: 3.068250  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 16.4%, Val - loss: 2.974618;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=2 ------------------------------------------\n",
            "\n",
            "Training loss: 2.785510  [  128/ 8582]\n",
            "Training loss: 2.710176  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 27.4%, Val - loss: 2.684571;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=3 ------------------------------------------\n",
            "\n",
            "Training loss: 2.203459  [  128/ 8582]\n",
            "Training loss: 2.122126  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 46.2%, Val - loss: 2.255593;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=4 ------------------------------------------\n",
            "\n",
            "Training loss: 1.700324  [  128/ 8582]\n",
            "Training loss: 1.508944  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 70.3%, Val - loss: 1.707742;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=5 ------------------------------------------\n",
            "\n",
            "Training loss: 1.405478  [  128/ 8582]\n",
            "Training loss: 0.989554  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 81.7%, Val - loss: 1.339637;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=6 ------------------------------------------\n",
            "\n",
            "Training loss: 0.801483  [  128/ 8582]\n",
            "Training loss: 0.641352  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 80.8%, Val - loss: 1.034544;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=7 ------------------------------------------\n",
            "\n",
            "Training loss: 0.677508  [  128/ 8582]\n",
            "Training loss: 0.483102  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 87.3%, Val - loss: 0.884363;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=8 ------------------------------------------\n",
            "\n",
            "Training loss: 0.481107  [  128/ 8582]\n",
            "Training loss: 0.353601  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 87.4%, Val - loss: 0.761978;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=9 ------------------------------------------\n",
            "\n",
            "Training loss: 0.397075  [  128/ 8582]\n",
            "Training loss: 0.312630  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 89.5%, Val - loss: 0.604163;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=10 ------------------------------------------\n",
            "\n",
            "Training loss: 0.316544  [  128/ 8582]\n",
            "Training loss: 0.234136  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.2%, Val - loss: 0.559931;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=11 ------------------------------------------\n",
            "\n",
            "Training loss: 0.270880  [  128/ 8582]\n",
            "Training loss: 0.200459  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.6%, Val - loss: 0.526055;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=12 ------------------------------------------\n",
            "\n",
            "Training loss: 0.237812  [  128/ 8582]\n",
            "Training loss: 0.174725  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.2%, Val - loss: 0.462808;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=13 ------------------------------------------\n",
            "\n",
            "Training loss: 0.180325  [  128/ 8582]\n",
            "Training loss: 0.140901  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.1%, Val - loss: 0.449124;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=14 ------------------------------------------\n",
            "\n",
            "Training loss: 0.168009  [  128/ 8582]\n",
            "Training loss: 0.140628  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.7%, Val - loss: 0.398339;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=15 ------------------------------------------\n",
            "\n",
            "Training loss: 0.223972  [  128/ 8582]\n",
            "Training loss: 0.123765  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.7%, Val - loss: 0.411431;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=16 ------------------------------------------\n",
            "\n",
            "Training loss: 0.158100  [  128/ 8582]\n",
            "Training loss: 0.101933  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.0%, Val - loss: 0.379684;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=17 ------------------------------------------\n",
            "\n",
            "Training loss: 0.205564  [  128/ 8582]\n",
            "Training loss: 0.097258  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.1%, Val - loss: 0.348665;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=18 ------------------------------------------\n",
            "\n",
            "Training loss: 0.160533  [  128/ 8582]\n",
            "Training loss: 0.090042  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.0%, Val - loss: 0.364612;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=19 ------------------------------------------\n",
            "\n",
            "Training loss: 0.142207  [  128/ 8582]\n",
            "Training loss: 0.086276  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.5%, Val - loss: 0.357715;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=20 ------------------------------------------\n",
            "\n",
            "Training loss: 0.158480  [  128/ 8582]\n",
            "Training loss: 0.056155  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.1%, Val - loss: 0.324346;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=21 ------------------------------------------\n",
            "\n",
            "Training loss: 0.089871  [  128/ 8582]\n",
            "Training loss: 0.059585  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.4%, Val - loss: 0.320935;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=22 ------------------------------------------\n",
            "\n",
            "Training loss: 0.096651  [  128/ 8582]\n",
            "Training loss: 0.085955  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.4%, Val - loss: 0.343324;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=23 ------------------------------------------\n",
            "\n",
            "Training loss: 0.113275  [  128/ 8582]\n",
            "Training loss: 0.080020  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.6%, Val - loss: 0.318279;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=24 ------------------------------------------\n",
            "\n",
            "Training loss: 0.093962  [  128/ 8582]\n",
            "Training loss: 0.068878  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.6%, Val - loss: 0.316829;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=25 ------------------------------------------\n",
            "\n",
            "Training loss: 0.102173  [  128/ 8582]\n",
            "Training loss: 0.052562  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.7%, Val - loss: 0.298435;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=26 ------------------------------------------\n",
            "\n",
            "Training loss: 0.097218  [  128/ 8582]\n",
            "Training loss: 0.039369  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.8%, Val - loss: 0.278305;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=27 ------------------------------------------\n",
            "\n",
            "Training loss: 0.083354  [  128/ 8582]\n",
            "Training loss: 0.030948  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.9%, Val - loss: 0.295753;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=28 ------------------------------------------\n",
            "\n",
            "Training loss: 0.118358  [  128/ 8582]\n",
            "Training loss: 0.046428  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.2%, Val - loss: 0.267041;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=29 ------------------------------------------\n",
            "\n",
            "Training loss: 0.062139  [  128/ 8582]\n",
            "Training loss: 0.043418  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 95.0%, Val - loss: 0.276528;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=30 ------------------------------------------\n",
            "\n",
            "Training loss: 0.062544  [  128/ 8582]\n",
            "Training loss: 0.047772  [ 6528/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 08:30:15,241] Trial 0 finished with value: 0.9276780859162589 and parameters: {'nr_conv_blocks': 3, 'out_channels_0': 103, 'out_channels_1': 108, 'out_channels_2': 56, 'dropout_rate_0': 0.09311226923965477, 'dropout_rate_1': 0.5274190264337794, 'dropout_rate_2': 0.5318355104524414, 'nr_fc_layers': 4, 'fc_size_0': 1061, 'fc_size_1': 1296, 'fc_size_2': 282, 'fc_size_3': 335, 'dropout_rate_fc_0': 0.026787442130207516, 'dropout_rate_fc_1': 0.21988561645437563, 'dropout_rate_fc_2': 0.531925035703286, 'dropout_rate_fc_3': 0.08818594703494743, 'optimizer': 'Adam', 'lr': 0.00032505379410407005, 'batch_size': 128}. Best is trial 0 with value: 0.9276780859162589.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 92.8%, Val - loss: 0.328272;\n",
            "\n",
            "#############################################\n",
            "#               Finished trial - 0/30               #\n",
            "#############################################\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 1               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5058774224485724, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(100, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.6856834908557528, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(116, 109, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.0003212958848130998, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(109, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.16958014090058512, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=3392, out_features=1282, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5807133607799305, inplace=False)\n",
            "    (3): Linear(in_features=1282, out_features=2018, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.3699963511281626, inplace=False)\n",
            "    (6): Linear(in_features=2018, out_features=900, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.18738834190169273, inplace=False)\n",
            "    (9): Linear(in_features=900, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 100, 128, 128]           1,000\n",
            "              ReLU-2        [-1, 100, 128, 128]               0\n",
            "           Dropout-3        [-1, 100, 128, 128]               0\n",
            "         MaxPool2d-4          [-1, 100, 64, 64]               0\n",
            "         ConvBlock-5          [-1, 100, 64, 64]               0\n",
            "            Conv2d-6          [-1, 116, 64, 64]         104,516\n",
            "              ReLU-7          [-1, 116, 64, 64]               0\n",
            "           Dropout-8          [-1, 116, 64, 64]               0\n",
            "         MaxPool2d-9          [-1, 116, 32, 32]               0\n",
            "        ConvBlock-10          [-1, 116, 32, 32]               0\n",
            "           Conv2d-11          [-1, 109, 32, 32]         113,905\n",
            "             ReLU-12          [-1, 109, 32, 32]               0\n",
            "          Dropout-13          [-1, 109, 32, 32]               0\n",
            "        MaxPool2d-14          [-1, 109, 16, 16]               0\n",
            "        ConvBlock-15          [-1, 109, 16, 16]               0\n",
            "           Conv2d-16           [-1, 53, 16, 16]          52,046\n",
            "             ReLU-17           [-1, 53, 16, 16]               0\n",
            "          Dropout-18           [-1, 53, 16, 16]               0\n",
            "        MaxPool2d-19             [-1, 53, 8, 8]               0\n",
            "        ConvBlock-20             [-1, 53, 8, 8]               0\n",
            "           Linear-21                 [-1, 1282]       4,349,826\n",
            "             ReLU-22                 [-1, 1282]               0\n",
            "          Dropout-23                 [-1, 1282]               0\n",
            "           Linear-24                 [-1, 2018]       2,589,094\n",
            "             ReLU-25                 [-1, 2018]               0\n",
            "          Dropout-26                 [-1, 2018]               0\n",
            "           Linear-27                  [-1, 900]       1,817,100\n",
            "             ReLU-28                  [-1, 900]               0\n",
            "          Dropout-29                  [-1, 900]               0\n",
            "           Linear-30                   [-1, 28]          25,228\n",
            "================================================================\n",
            "Total params: 9,052,715\n",
            "Trainable params: 9,052,715\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 59.88\n",
            "Params size (MB): 34.53\n",
            "Estimated Total Size (MB): 94.47\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.332539  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 4.8%, Val - loss: 3.334320;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=2 ------------------------------------------\n",
            "\n",
            "Training loss: 3.437047  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.332080;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=3 ------------------------------------------\n",
            "\n",
            "Training loss: 3.323757  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.331759;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=4 ------------------------------------------\n",
            "\n",
            "Training loss: 3.338001  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 4.8%, Val - loss: 3.331401;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=5 ------------------------------------------\n",
            "\n",
            "Training loss: 3.355236  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.331026;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=6 ------------------------------------------\n",
            "\n",
            "Training loss: 3.325695  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.331522;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=7 ------------------------------------------\n",
            "\n",
            "Training loss: 3.365439  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.331962;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=8 ------------------------------------------\n",
            "\n",
            "Training loss: 3.357591  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.331195;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=9 ------------------------------------------\n",
            "\n",
            "Training loss: 3.329702  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.331172;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=10 ------------------------------------------\n",
            "\n",
            "Training loss: 3.333677  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.331150;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=11 ------------------------------------------\n",
            "\n",
            "Training loss: 3.346631  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.330982;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=12 ------------------------------------------\n",
            "\n",
            "Training loss: 3.321116  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.330720;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=13 ------------------------------------------\n",
            "\n",
            "Training loss: 3.321692  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.331309;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=14 ------------------------------------------\n",
            "\n",
            "Training loss: 3.329940  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.331239;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=15 ------------------------------------------\n",
            "\n",
            "Training loss: 3.328798  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.325710;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=16 ------------------------------------------\n",
            "\n",
            "Training loss: 3.299518  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 4.7%, Val - loss: 3.287330;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=17 ------------------------------------------\n",
            "\n",
            "Training loss: 3.266598  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.289927;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=18 ------------------------------------------\n",
            "\n",
            "Training loss: 3.231659  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.610759;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=19 ------------------------------------------\n",
            "\n",
            "Training loss: 3.186566  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.9%, Val - loss: 3.264426;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=20 ------------------------------------------\n",
            "\n",
            "Training loss: 3.173485  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.407171;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=21 ------------------------------------------\n",
            "\n",
            "Training loss: 3.150240  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 2.8%, Val - loss: 3.531891;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=22 ------------------------------------------\n",
            "\n",
            "Training loss: 3.145795  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.699350;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=23 ------------------------------------------\n",
            "\n",
            "Training loss: 3.076550  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.588013;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=24 ------------------------------------------\n",
            "\n",
            "Training loss: 3.020578  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 4.8%, Val - loss: 3.660861;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=25 ------------------------------------------\n",
            "\n",
            "Training loss: 3.028378  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 6.2%, Val - loss: 3.525767;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=26 ------------------------------------------\n",
            "\n",
            "Training loss: 3.021972  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 4.8%, Val - loss: 3.911262;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=27 ------------------------------------------\n",
            "\n",
            "Training loss: 3.045260  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 5.9%, Val - loss: 3.591747;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=28 ------------------------------------------\n",
            "\n",
            "Training loss: 3.092040  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.591720;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=29 ------------------------------------------\n",
            "\n",
            "Training loss: 2.995939  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.687107;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=30 ------------------------------------------\n",
            "\n",
            "Training loss: 3.017943  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 5.2%, Val - loss: 3.385620;\n",
            "\n",
            "#############################################\n",
            "#               Finished trial - 1/30               #\n",
            "#############################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 08:45:04,872] Trial 1 finished with value: 0.052202283849918436 and parameters: {'nr_conv_blocks': 4, 'out_channels_0': 100, 'out_channels_1': 116, 'out_channels_2': 109, 'out_channels_3': 53, 'dropout_rate_0': 0.5058774224485724, 'dropout_rate_1': 0.6856834908557528, 'dropout_rate_2': 0.0003212958848130998, 'dropout_rate_3': 0.16958014090058512, 'nr_fc_layers': 3, 'fc_size_0': 1282, 'fc_size_1': 2018, 'fc_size_2': 900, 'dropout_rate_fc_0': 0.5807133607799305, 'dropout_rate_fc_1': 0.3699963511281626, 'dropout_rate_fc_2': 0.18738834190169273, 'optimizer': 'RMSprop', 'lr': 0.0075131027483900156, 'batch_size': 256}. Best is trial 0 with value: 0.9276780859162589.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 2               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.3737849496746045, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(40, 87, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.2011224096212939, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=89088, out_features=1843, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.3825537036779852, inplace=False)\n",
            "    (3): Linear(in_features=1843, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 40, 128, 128]             400\n",
            "              ReLU-2         [-1, 40, 128, 128]               0\n",
            "           Dropout-3         [-1, 40, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 40, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 40, 64, 64]               0\n",
            "            Conv2d-6           [-1, 87, 64, 64]          31,407\n",
            "              ReLU-7           [-1, 87, 64, 64]               0\n",
            "           Dropout-8           [-1, 87, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 87, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 87, 32, 32]               0\n",
            "           Linear-11                 [-1, 1843]     164,191,027\n",
            "             ReLU-12                 [-1, 1843]               0\n",
            "          Dropout-13                 [-1, 1843]               0\n",
            "           Linear-14                   [-1, 28]          51,632\n",
            "================================================================\n",
            "Total params: 164,274,466\n",
            "Trainable params: 164,274,466\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 27.06\n",
            "Params size (MB): 626.66\n",
            "Estimated Total Size (MB): 653.78\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.327126  [   64/ 8582]\n",
            "Training loss: 3.020188  [ 3264/ 8582]\n",
            "Training loss: 2.751748  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 13.6%, Val - loss: 3.019263;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=2 ------------------------------------------\n",
            "\n",
            "Training loss: 3.682204  [   64/ 8582]\n",
            "Training loss: 2.323326  [ 3264/ 8582]\n",
            "Training loss: 2.089879  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 18.3%, Val - loss: 2.805627;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=3 ------------------------------------------\n",
            "\n",
            "Training loss: 3.621371  [   64/ 8582]\n",
            "Training loss: 1.744285  [ 3264/ 8582]\n",
            "Training loss: 1.512776  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 24.9%, Val - loss: 2.615974;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=4 ------------------------------------------\n",
            "\n",
            "Training loss: 3.669279  [   64/ 8582]\n",
            "Training loss: 1.300035  [ 3264/ 8582]\n",
            "Training loss: 1.162415  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 30.9%, Val - loss: 2.210742;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=5 ------------------------------------------\n",
            "\n",
            "Training loss: 2.834965  [   64/ 8582]\n",
            "Training loss: 1.039602  [ 3264/ 8582]\n",
            "Training loss: 0.916960  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 49.6%, Val - loss: 1.789518;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=6 ------------------------------------------\n",
            "\n",
            "Training loss: 2.079858  [   64/ 8582]\n",
            "Training loss: 0.817003  [ 3264/ 8582]\n",
            "Training loss: 0.759033  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 72.1%, Val - loss: 1.393710;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=7 ------------------------------------------\n",
            "\n",
            "Training loss: 1.322463  [   64/ 8582]\n",
            "Training loss: 0.728575  [ 3264/ 8582]\n",
            "Training loss: 0.666287  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 59.1%, Val - loss: 1.584757;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=8 ------------------------------------------\n",
            "\n",
            "Training loss: 1.844947  [   64/ 8582]\n",
            "Training loss: 0.643121  [ 3264/ 8582]\n",
            "Training loss: 0.539365  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 77.5%, Val - loss: 1.326942;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=9 ------------------------------------------\n",
            "\n",
            "Training loss: 1.347507  [   64/ 8582]\n",
            "Training loss: 0.553297  [ 3264/ 8582]\n",
            "Training loss: 0.492776  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 81.0%, Val - loss: 1.016428;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=10 ------------------------------------------\n",
            "\n",
            "Training loss: 0.784139  [   64/ 8582]\n",
            "Training loss: 0.474355  [ 3264/ 8582]\n",
            "Training loss: 0.436435  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 68.7%, Val - loss: 1.459830;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=11 ------------------------------------------\n",
            "\n",
            "Training loss: 1.656726  [   64/ 8582]\n",
            "Training loss: 0.448934  [ 3264/ 8582]\n",
            "Training loss: 0.401430  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 84.0%, Val - loss: 0.919782;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=12 ------------------------------------------\n",
            "\n",
            "Training loss: 0.562460  [   64/ 8582]\n",
            "Training loss: 0.415257  [ 3264/ 8582]\n",
            "Training loss: 0.347297  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 76.8%, Val - loss: 1.169933;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=13 ------------------------------------------\n",
            "\n",
            "Training loss: 1.144340  [   64/ 8582]\n",
            "Training loss: 0.376750  [ 3264/ 8582]\n",
            "Training loss: 0.324212  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 83.3%, Val - loss: 1.053278;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=14 ------------------------------------------\n",
            "\n",
            "Training loss: 0.925916  [   64/ 8582]\n",
            "Training loss: 0.364108  [ 3264/ 8582]\n",
            "Training loss: 0.300732  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 84.0%, Val - loss: 1.030527;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=15 ------------------------------------------\n",
            "\n",
            "Training loss: 1.001188  [   64/ 8582]\n",
            "Training loss: 0.332958  [ 3264/ 8582]\n",
            "Training loss: 0.278202  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 84.4%, Val - loss: 0.956258;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=16 ------------------------------------------\n",
            "\n",
            "Training loss: 0.751896  [   64/ 8582]\n",
            "Training loss: 0.305665  [ 3264/ 8582]\n",
            "Training loss: 0.262831  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 83.2%, Val - loss: 1.223282;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=17 ------------------------------------------\n",
            "\n",
            "Training loss: 1.306227  [   64/ 8582]\n",
            "Training loss: 0.292473  [ 3264/ 8582]\n",
            "Training loss: 0.242023  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 85.5%, Val - loss: 0.738511;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=18 ------------------------------------------\n",
            "\n",
            "Training loss: 0.565340  [   64/ 8582]\n",
            "Training loss: 0.254316  [ 3264/ 8582]\n",
            "Training loss: 0.225259  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 85.3%, Val - loss: 0.989400;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=19 ------------------------------------------\n",
            "\n",
            "Training loss: 0.805964  [   64/ 8582]\n",
            "Training loss: 0.235684  [ 3264/ 8582]\n",
            "Training loss: 0.202738  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 86.2%, Val - loss: 0.690261;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=20 ------------------------------------------\n",
            "\n",
            "Training loss: 0.460464  [   64/ 8582]\n",
            "Training loss: 0.215486  [ 3264/ 8582]\n",
            "Training loss: 0.238632  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 80.4%, Val - loss: 1.063577;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=21 ------------------------------------------\n",
            "\n",
            "Training loss: 0.904648  [   64/ 8582]\n",
            "Training loss: 0.230759  [ 3264/ 8582]\n",
            "Training loss: 0.193072  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 85.2%, Val - loss: 1.107993;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=22 ------------------------------------------\n",
            "\n",
            "Training loss: 1.246192  [   64/ 8582]\n",
            "Training loss: 0.229662  [ 3264/ 8582]\n",
            "Training loss: 0.187720  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 84.7%, Val - loss: 0.977514;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=23 ------------------------------------------\n",
            "\n",
            "Training loss: 0.920670  [   64/ 8582]\n",
            "Training loss: 0.202321  [ 3264/ 8582]\n",
            "Training loss: 0.169918  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 87.7%, Val - loss: 0.563711;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=24 ------------------------------------------\n",
            "\n",
            "Training loss: 0.376848  [   64/ 8582]\n",
            "Training loss: 0.161670  [ 3264/ 8582]\n",
            "Training loss: 0.170062  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 85.2%, Val - loss: 0.863825;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=25 ------------------------------------------\n",
            "\n",
            "Training loss: 0.934022  [   64/ 8582]\n",
            "Training loss: 0.184708  [ 3264/ 8582]\n",
            "Training loss: 0.150662  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 86.6%, Val - loss: 0.655324;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=26 ------------------------------------------\n",
            "\n",
            "Training loss: 0.500641  [   64/ 8582]\n",
            "Training loss: 0.156697  [ 3264/ 8582]\n",
            "Training loss: 0.157879  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 87.6%, Val - loss: 0.590021;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=27 ------------------------------------------\n",
            "\n",
            "Training loss: 0.444406  [   64/ 8582]\n",
            "Training loss: 0.135036  [ 3264/ 8582]\n",
            "Training loss: 0.155524  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 86.1%, Val - loss: 0.743804;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=28 ------------------------------------------\n",
            "\n",
            "Training loss: 0.861826  [   64/ 8582]\n",
            "Training loss: 0.170704  [ 3264/ 8582]\n",
            "Training loss: 0.131500  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 87.5%, Val - loss: 0.666794;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=29 ------------------------------------------\n",
            "\n",
            "Training loss: 0.563042  [   64/ 8582]\n",
            "Training loss: 0.133053  [ 3264/ 8582]\n",
            "Training loss: 0.127603  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 84.2%, Val - loss: 1.207340;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=30 ------------------------------------------\n",
            "\n",
            "Training loss: 1.576767  [   64/ 8582]\n",
            "Training loss: 0.189756  [ 3264/ 8582]\n",
            "Training loss: 0.128121  [ 6464/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 08:59:43,360] Trial 2 finished with value: 0.8466557911908646 and parameters: {'nr_conv_blocks': 2, 'out_channels_0': 40, 'out_channels_1': 87, 'dropout_rate_0': 0.3737849496746045, 'dropout_rate_1': 0.2011224096212939, 'nr_fc_layers': 1, 'fc_size_0': 1843, 'dropout_rate_fc_0': 0.3825537036779852, 'optimizer': 'SGD', 'lr': 0.007115504859689682, 'batch_size': 64}. Best is trial 0 with value: 0.9276780859162589.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 84.7%, Val - loss: 1.131743;\n",
            "\n",
            "#############################################\n",
            "#               Finished trial - 2/30               #\n",
            "#############################################\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 3               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.2600750701045177, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(90, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5511384039955239, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(61, 81, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.2768973776544742, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(81, 94, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.12301003766717822, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=6016, out_features=259, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5886458883013169, inplace=False)\n",
            "    (3): Linear(in_features=259, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 90, 128, 128]             900\n",
            "              ReLU-2         [-1, 90, 128, 128]               0\n",
            "           Dropout-3         [-1, 90, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 90, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 90, 64, 64]               0\n",
            "            Conv2d-6           [-1, 61, 64, 64]          49,471\n",
            "              ReLU-7           [-1, 61, 64, 64]               0\n",
            "           Dropout-8           [-1, 61, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 61, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 61, 32, 32]               0\n",
            "           Conv2d-11           [-1, 81, 32, 32]          44,550\n",
            "             ReLU-12           [-1, 81, 32, 32]               0\n",
            "          Dropout-13           [-1, 81, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 81, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 81, 16, 16]               0\n",
            "           Conv2d-16           [-1, 94, 16, 16]          68,620\n",
            "             ReLU-17           [-1, 94, 16, 16]               0\n",
            "          Dropout-18           [-1, 94, 16, 16]               0\n",
            "        MaxPool2d-19             [-1, 94, 8, 8]               0\n",
            "        ConvBlock-20             [-1, 94, 8, 8]               0\n",
            "           Linear-21                  [-1, 259]       1,558,403\n",
            "             ReLU-22                  [-1, 259]               0\n",
            "          Dropout-23                  [-1, 259]               0\n",
            "           Linear-24                   [-1, 28]           7,280\n",
            "================================================================\n",
            "Total params: 1,729,224\n",
            "Trainable params: 1,729,224\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 48.91\n",
            "Params size (MB): 6.60\n",
            "Estimated Total Size (MB): 55.57\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.317680  [   32/ 8582]\n",
            "Training loss: 3.220624  [ 1632/ 8582]\n",
            "Training loss: 3.032589  [ 3232/ 8582]\n",
            "Training loss: 2.973881  [ 4832/ 8582]\n",
            "Training loss: 2.903618  [ 6432/ 8582]\n",
            "Training loss: 2.843160  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 13.5%, Val - loss: 3.037132;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=2 ------------------------------------------\n",
            "\n",
            "Training loss: 2.645108  [   32/ 8582]\n",
            "Training loss: 2.751060  [ 1632/ 8582]\n",
            "Training loss: 2.695465  [ 3232/ 8582]\n",
            "Training loss: 2.616644  [ 4832/ 8582]\n",
            "Training loss: 2.493631  [ 6432/ 8582]\n",
            "Training loss: 2.381835  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 41.2%, Val - loss: 2.873921;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=3 ------------------------------------------\n",
            "\n",
            "Training loss: 2.340997  [   32/ 8582]\n",
            "Training loss: 2.219985  [ 1632/ 8582]\n",
            "Training loss: 2.111950  [ 3232/ 8582]\n",
            "Training loss: 1.935755  [ 4832/ 8582]\n",
            "Training loss: 1.848685  [ 6432/ 8582]\n",
            "Training loss: 1.752050  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 44.8%, Val - loss: 2.315060;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=4 ------------------------------------------\n",
            "\n",
            "Training loss: 2.103154  [   32/ 8582]\n",
            "Training loss: 1.576291  [ 1632/ 8582]\n",
            "Training loss: 1.490215  [ 3232/ 8582]\n",
            "Training loss: 1.397016  [ 4832/ 8582]\n",
            "Training loss: 1.355654  [ 6432/ 8582]\n",
            "Training loss: 1.260998  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 61.1%, Val - loss: 2.137184;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=5 ------------------------------------------\n",
            "\n",
            "Training loss: 1.620919  [   32/ 8582]\n",
            "Training loss: 1.170232  [ 1632/ 8582]\n",
            "Training loss: 1.109946  [ 3232/ 8582]\n",
            "Training loss: 1.002971  [ 4832/ 8582]\n",
            "Training loss: 0.976587  [ 6432/ 8582]\n",
            "Training loss: 0.920411  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 80.6%, Val - loss: 1.551073;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=6 ------------------------------------------\n",
            "\n",
            "Training loss: 1.344029  [   32/ 8582]\n",
            "Training loss: 0.910781  [ 1632/ 8582]\n",
            "Training loss: 0.841973  [ 3232/ 8582]\n",
            "Training loss: 0.792919  [ 4832/ 8582]\n",
            "Training loss: 0.768427  [ 6432/ 8582]\n",
            "Training loss: 0.739510  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 72.6%, Val - loss: 1.519711;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=7 ------------------------------------------\n",
            "\n",
            "Training loss: 1.116844  [   32/ 8582]\n",
            "Training loss: 0.733921  [ 1632/ 8582]\n",
            "Training loss: 0.701744  [ 3232/ 8582]\n",
            "Training loss: 0.636763  [ 4832/ 8582]\n",
            "Training loss: 0.636405  [ 6432/ 8582]\n",
            "Training loss: 0.653793  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 64.1%, Val - loss: 1.790470;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=8 ------------------------------------------\n",
            "\n",
            "Training loss: 1.202911  [   32/ 8582]\n",
            "Training loss: 0.652533  [ 1632/ 8582]\n",
            "Training loss: 0.574711  [ 3232/ 8582]\n",
            "Training loss: 0.563850  [ 4832/ 8582]\n",
            "Training loss: 0.530777  [ 6432/ 8582]\n",
            "Training loss: 0.533162  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 86.8%, Val - loss: 1.168923;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=9 ------------------------------------------\n",
            "\n",
            "Training loss: 0.595961  [   32/ 8582]\n",
            "Training loss: 0.519344  [ 1632/ 8582]\n",
            "Training loss: 0.478685  [ 3232/ 8582]\n",
            "Training loss: 0.438166  [ 4832/ 8582]\n",
            "Training loss: 0.465039  [ 6432/ 8582]\n",
            "Training loss: 0.469920  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 88.7%, Val - loss: 0.880476;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=10 ------------------------------------------\n",
            "\n",
            "Training loss: 0.399684  [   32/ 8582]\n",
            "Training loss: 0.395772  [ 1632/ 8582]\n",
            "Training loss: 0.401801  [ 3232/ 8582]\n",
            "Training loss: 0.405529  [ 4832/ 8582]\n",
            "Training loss: 0.418811  [ 6432/ 8582]\n",
            "Training loss: 0.372678  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.5%, Val - loss: 0.848963;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=11 ------------------------------------------\n",
            "\n",
            "Training loss: 0.202654  [   32/ 8582]\n",
            "Training loss: 0.357782  [ 1632/ 8582]\n",
            "Training loss: 0.377732  [ 3232/ 8582]\n",
            "Training loss: 0.363622  [ 4832/ 8582]\n",
            "Training loss: 0.328832  [ 6432/ 8582]\n",
            "Training loss: 0.376970  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.9%, Val - loss: 0.797662;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=12 ------------------------------------------\n",
            "\n",
            "Training loss: 0.242885  [   32/ 8582]\n",
            "Training loss: 0.295551  [ 1632/ 8582]\n",
            "Training loss: 0.332907  [ 3232/ 8582]\n",
            "Training loss: 0.311155  [ 4832/ 8582]\n",
            "Training loss: 0.315489  [ 6432/ 8582]\n",
            "Training loss: 0.325072  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.8%, Val - loss: 0.694330;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=13 ------------------------------------------\n",
            "\n",
            "Training loss: 0.250500  [   32/ 8582]\n",
            "Training loss: 0.256393  [ 1632/ 8582]\n",
            "Training loss: 0.303526  [ 3232/ 8582]\n",
            "Training loss: 0.265304  [ 4832/ 8582]\n",
            "Training loss: 0.293944  [ 6432/ 8582]\n",
            "Training loss: 0.289444  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.5%, Val - loss: 0.629492;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=14 ------------------------------------------\n",
            "\n",
            "Training loss: 0.207743  [   32/ 8582]\n",
            "Training loss: 0.248977  [ 1632/ 8582]\n",
            "Training loss: 0.287620  [ 3232/ 8582]\n",
            "Training loss: 0.253121  [ 4832/ 8582]\n",
            "Training loss: 0.260389  [ 6432/ 8582]\n",
            "Training loss: 0.235790  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.5%, Val - loss: 0.562313;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=15 ------------------------------------------\n",
            "\n",
            "Training loss: 0.233047  [   32/ 8582]\n",
            "Training loss: 0.240279  [ 1632/ 8582]\n",
            "Training loss: 0.264964  [ 3232/ 8582]\n",
            "Training loss: 0.241793  [ 4832/ 8582]\n",
            "Training loss: 0.225392  [ 6432/ 8582]\n",
            "Training loss: 0.236643  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 87.1%, Val - loss: 0.628855;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=16 ------------------------------------------\n",
            "\n",
            "Training loss: 0.720116  [   32/ 8582]\n",
            "Training loss: 0.223452  [ 1632/ 8582]\n",
            "Training loss: 0.241544  [ 3232/ 8582]\n",
            "Training loss: 0.209487  [ 4832/ 8582]\n",
            "Training loss: 0.219615  [ 6432/ 8582]\n",
            "Training loss: 0.206778  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.9%, Val - loss: 0.520226;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=17 ------------------------------------------\n",
            "\n",
            "Training loss: 0.112360  [   32/ 8582]\n",
            "Training loss: 0.172688  [ 1632/ 8582]\n",
            "Training loss: 0.215450  [ 3232/ 8582]\n",
            "Training loss: 0.171461  [ 4832/ 8582]\n",
            "Training loss: 0.213747  [ 6432/ 8582]\n",
            "Training loss: 0.186227  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.6%, Val - loss: 0.548217;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=18 ------------------------------------------\n",
            "\n",
            "Training loss: 0.146947  [   32/ 8582]\n",
            "Training loss: 0.183251  [ 1632/ 8582]\n",
            "Training loss: 0.226269  [ 3232/ 8582]\n",
            "Training loss: 0.186272  [ 4832/ 8582]\n",
            "Training loss: 0.216839  [ 6432/ 8582]\n",
            "Training loss: 0.152663  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.7%, Val - loss: 0.471130;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=19 ------------------------------------------\n",
            "\n",
            "Training loss: 0.179104  [   32/ 8582]\n",
            "Training loss: 0.200105  [ 1632/ 8582]\n",
            "Training loss: 0.163176  [ 3232/ 8582]\n",
            "Training loss: 0.147565  [ 4832/ 8582]\n",
            "Training loss: 0.184237  [ 6432/ 8582]\n",
            "Training loss: 0.177542  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.4%, Val - loss: 0.480197;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=20 ------------------------------------------\n",
            "\n",
            "Training loss: 0.060984  [   32/ 8582]\n",
            "Training loss: 0.148331  [ 1632/ 8582]\n",
            "Training loss: 0.177774  [ 3232/ 8582]\n",
            "Training loss: 0.121314  [ 4832/ 8582]\n",
            "Training loss: 0.159145  [ 6432/ 8582]\n",
            "Training loss: 0.135850  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.1%, Val - loss: 0.403871;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=21 ------------------------------------------\n",
            "\n",
            "Training loss: 0.144260  [   32/ 8582]\n",
            "Training loss: 0.154047  [ 1632/ 8582]\n",
            "Training loss: 0.176356  [ 3232/ 8582]\n",
            "Training loss: 0.113099  [ 4832/ 8582]\n",
            "Training loss: 0.159861  [ 6432/ 8582]\n",
            "Training loss: 0.164512  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.8%, Val - loss: 0.449526;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=22 ------------------------------------------\n",
            "\n",
            "Training loss: 0.281718  [   32/ 8582]\n",
            "Training loss: 0.165117  [ 1632/ 8582]\n",
            "Training loss: 0.191074  [ 3232/ 8582]\n",
            "Training loss: 0.142111  [ 4832/ 8582]\n",
            "Training loss: 0.152274  [ 6432/ 8582]\n",
            "Training loss: 0.135942  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.5%, Val - loss: 0.395329;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=23 ------------------------------------------\n",
            "\n",
            "Training loss: 0.071006  [   32/ 8582]\n",
            "Training loss: 0.112215  [ 1632/ 8582]\n",
            "Training loss: 0.158163  [ 3232/ 8582]\n",
            "Training loss: 0.162809  [ 4832/ 8582]\n",
            "Training loss: 0.124009  [ 6432/ 8582]\n",
            "Training loss: 0.135164  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.2%, Val - loss: 0.376637;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=24 ------------------------------------------\n",
            "\n",
            "Training loss: 0.155845  [   32/ 8582]\n",
            "Training loss: 0.117009  [ 1632/ 8582]\n",
            "Training loss: 0.152060  [ 3232/ 8582]\n",
            "Training loss: 0.125118  [ 4832/ 8582]\n",
            "Training loss: 0.121846  [ 6432/ 8582]\n",
            "Training loss: 0.126319  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.5%, Val - loss: 0.386087;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=25 ------------------------------------------\n",
            "\n",
            "Training loss: 0.162182  [   32/ 8582]\n",
            "Training loss: 0.093678  [ 1632/ 8582]\n",
            "Training loss: 0.137782  [ 3232/ 8582]\n",
            "Training loss: 0.093182  [ 4832/ 8582]\n",
            "Training loss: 0.115989  [ 6432/ 8582]\n",
            "Training loss: 0.093234  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.7%, Val - loss: 0.407804;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=26 ------------------------------------------\n",
            "\n",
            "Training loss: 0.261579  [   32/ 8582]\n",
            "Training loss: 0.122520  [ 1632/ 8582]\n",
            "Training loss: 0.140163  [ 3232/ 8582]\n",
            "Training loss: 0.109131  [ 4832/ 8582]\n",
            "Training loss: 0.100630  [ 6432/ 8582]\n",
            "Training loss: 0.145655  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.6%, Val - loss: 0.355658;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=27 ------------------------------------------\n",
            "\n",
            "Training loss: 0.101588  [   32/ 8582]\n",
            "Training loss: 0.112974  [ 1632/ 8582]\n",
            "Training loss: 0.092863  [ 3232/ 8582]\n",
            "Training loss: 0.100697  [ 4832/ 8582]\n",
            "Training loss: 0.133957  [ 6432/ 8582]\n",
            "Training loss: 0.120748  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.7%, Val - loss: 0.361739;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=28 ------------------------------------------\n",
            "\n",
            "Training loss: 0.154737  [   32/ 8582]\n",
            "Training loss: 0.095930  [ 1632/ 8582]\n",
            "Training loss: 0.100811  [ 3232/ 8582]\n",
            "Training loss: 0.103486  [ 4832/ 8582]\n",
            "Training loss: 0.125806  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.7%, Val - loss: 0.364849;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=29 ------------------------------------------\n",
            "\n",
            "Training loss: 0.068247  [   32/ 8582]\n",
            "Training loss: 0.118856  [ 1632/ 8582]\n",
            "Training loss: 0.101632  [ 3232/ 8582]\n",
            "Training loss: 0.076063  [ 4832/ 8582]\n",
            "Training loss: 0.107857  [ 6432/ 8582]\n",
            "Training loss: 0.110995  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.1%, Val - loss: 0.319754;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=30 ------------------------------------------\n",
            "\n",
            "Training loss: 0.180160  [   32/ 8582]\n",
            "Training loss: 0.091404  [ 1632/ 8582]\n",
            "Training loss: 0.127410  [ 3232/ 8582]\n",
            "Training loss: 0.100313  [ 4832/ 8582]\n",
            "Training loss: 0.106058  [ 6432/ 8582]\n",
            "Training loss: 0.104514  [ 8032/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 09:14:20,549] Trial 3 finished with value: 0.9467101685698749 and parameters: {'nr_conv_blocks': 4, 'out_channels_0': 90, 'out_channels_1': 61, 'out_channels_2': 81, 'out_channels_3': 94, 'dropout_rate_0': 0.2600750701045177, 'dropout_rate_1': 0.5511384039955239, 'dropout_rate_2': 0.2768973776544742, 'dropout_rate_3': 0.12301003766717822, 'nr_fc_layers': 1, 'fc_size_0': 259, 'dropout_rate_fc_0': 0.5886458883013169, 'optimizer': 'SGD', 'lr': 0.021218488681107886, 'batch_size': 32}. Best is trial 3 with value: 0.9467101685698749.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 94.7%, Val - loss: 0.332693;\n",
            "\n",
            "#############################################\n",
            "#               Finished trial - 3/30               #\n",
            "#############################################\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 4               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 68, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.061118579786100156, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(68, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.42538894362915386, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=88064, out_features=1008, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5791857847845037, inplace=False)\n",
            "    (3): Linear(in_features=1008, out_features=2024, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.11052848367721012, inplace=False)\n",
            "    (6): Linear(in_features=2024, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 68, 128, 128]             680\n",
            "              ReLU-2         [-1, 68, 128, 128]               0\n",
            "           Dropout-3         [-1, 68, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 68, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 68, 64, 64]               0\n",
            "            Conv2d-6           [-1, 86, 64, 64]          52,718\n",
            "              ReLU-7           [-1, 86, 64, 64]               0\n",
            "           Dropout-8           [-1, 86, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 86, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 86, 32, 32]               0\n",
            "           Linear-11                 [-1, 1008]      88,769,520\n",
            "             ReLU-12                 [-1, 1008]               0\n",
            "          Dropout-13                 [-1, 1008]               0\n",
            "           Linear-14                 [-1, 2024]       2,042,216\n",
            "             ReLU-15                 [-1, 2024]               0\n",
            "          Dropout-16                 [-1, 2024]               0\n",
            "           Linear-17                   [-1, 28]          56,700\n",
            "================================================================\n",
            "Total params: 90,921,834\n",
            "Trainable params: 90,921,834\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 39.23\n",
            "Params size (MB): 346.84\n",
            "Estimated Total Size (MB): 386.13\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.335100  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 24.4%, Val - loss: 2.838456;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=2 ------------------------------------------\n",
            "\n",
            "Training loss: 2.716990  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 44.4%, Val - loss: 2.501124;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=3 ------------------------------------------\n",
            "\n",
            "Training loss: 2.353645  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 57.0%, Val - loss: 2.091410;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=4 ------------------------------------------\n",
            "\n",
            "Training loss: 1.860581  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 69.6%, Val - loss: 1.718020;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=5 ------------------------------------------\n",
            "\n",
            "Training loss: 1.448362  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 77.0%, Val - loss: 1.382718;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=6 ------------------------------------------\n",
            "\n",
            "Training loss: 1.137089  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 81.7%, Val - loss: 1.154320;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=7 ------------------------------------------\n",
            "\n",
            "Training loss: 0.945437  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 83.1%, Val - loss: 1.019219;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=8 ------------------------------------------\n",
            "\n",
            "Training loss: 0.882848  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 84.4%, Val - loss: 0.898156;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=9 ------------------------------------------\n",
            "\n",
            "Training loss: 0.768810  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 85.3%, Val - loss: 0.836716;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=10 ------------------------------------------\n",
            "\n",
            "Training loss: 0.698357  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 84.9%, Val - loss: 0.763681;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=11 ------------------------------------------\n",
            "\n",
            "Training loss: 0.597755  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 86.5%, Val - loss: 0.712505;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=12 ------------------------------------------\n",
            "\n",
            "Training loss: 0.588193  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 86.8%, Val - loss: 0.671799;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=13 ------------------------------------------\n",
            "\n",
            "Training loss: 0.619734  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 87.8%, Val - loss: 0.635839;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=14 ------------------------------------------\n",
            "\n",
            "Training loss: 0.502318  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 88.4%, Val - loss: 0.594688;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=15 ------------------------------------------\n",
            "\n",
            "Training loss: 0.506040  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 89.2%, Val - loss: 0.557106;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=16 ------------------------------------------\n",
            "\n",
            "Training loss: 0.400556  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 89.3%, Val - loss: 0.542515;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=17 ------------------------------------------\n",
            "\n",
            "Training loss: 0.422109  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.5%, Val - loss: 0.503080;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=18 ------------------------------------------\n",
            "\n",
            "Training loss: 0.422924  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.4%, Val - loss: 0.491646;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=19 ------------------------------------------\n",
            "\n",
            "Training loss: 0.344940  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.5%, Val - loss: 0.462907;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=20 ------------------------------------------\n",
            "\n",
            "Training loss: 0.357140  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.4%, Val - loss: 0.449083;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=21 ------------------------------------------\n",
            "\n",
            "Training loss: 0.284567  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.8%, Val - loss: 0.441911;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=22 ------------------------------------------\n",
            "\n",
            "Training loss: 0.333198  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.6%, Val - loss: 0.435594;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=23 ------------------------------------------\n",
            "\n",
            "Training loss: 0.295203  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.6%, Val - loss: 0.410779;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=24 ------------------------------------------\n",
            "\n",
            "Training loss: 0.283342  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.7%, Val - loss: 0.389984;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=25 ------------------------------------------\n",
            "\n",
            "Training loss: 0.277296  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.0%, Val - loss: 0.386764;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=26 ------------------------------------------\n",
            "\n",
            "Training loss: 0.271028  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.7%, Val - loss: 0.368953;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=27 ------------------------------------------\n",
            "\n",
            "Training loss: 0.264925  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.3%, Val - loss: 0.360732;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=28 ------------------------------------------\n",
            "\n",
            "Training loss: 0.233607  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.7%, Val - loss: 0.348947;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=29 ------------------------------------------\n",
            "\n",
            "Training loss: 0.225137  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.8%, Val - loss: 0.360075;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=30 ------------------------------------------\n",
            "\n",
            "Training loss: 0.211447  [  256/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 09:28:54,165] Trial 4 finished with value: 0.9233278955954323 and parameters: {'nr_conv_blocks': 2, 'out_channels_0': 68, 'out_channels_1': 86, 'dropout_rate_0': 0.061118579786100156, 'dropout_rate_1': 0.42538894362915386, 'nr_fc_layers': 2, 'fc_size_0': 1008, 'fc_size_1': 2024, 'dropout_rate_fc_0': 0.5791857847845037, 'dropout_rate_fc_1': 0.11052848367721012, 'optimizer': 'Adam', 'lr': 4.261372195739448e-05, 'batch_size': 256}. Best is trial 3 with value: 0.9467101685698749.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 92.3%, Val - loss: 0.339484;\n",
            "\n",
            "#############################################\n",
            "#               Finished trial - 4/30               #\n",
            "#############################################\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 5               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.018343004940264293, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(84, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.6206157222602162, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(42, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.3680568000847196, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=12800, out_features=1928, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.6872421147688582, inplace=False)\n",
            "    (3): Linear(in_features=1928, out_features=737, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.4792623316169729, inplace=False)\n",
            "    (6): Linear(in_features=737, out_features=531, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.10915982085286537, inplace=False)\n",
            "    (9): Linear(in_features=531, out_features=406, bias=True)\n",
            "    (10): ReLU()\n",
            "    (11): Dropout(p=0.09127597578601145, inplace=False)\n",
            "    (12): Linear(in_features=406, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 84, 128, 128]             840\n",
            "              ReLU-2         [-1, 84, 128, 128]               0\n",
            "           Dropout-3         [-1, 84, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 84, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 84, 64, 64]               0\n",
            "            Conv2d-6           [-1, 42, 64, 64]          31,794\n",
            "              ReLU-7           [-1, 42, 64, 64]               0\n",
            "           Dropout-8           [-1, 42, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 42, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 42, 32, 32]               0\n",
            "           Conv2d-11           [-1, 50, 32, 32]          18,950\n",
            "             ReLU-12           [-1, 50, 32, 32]               0\n",
            "          Dropout-13           [-1, 50, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 50, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 50, 16, 16]               0\n",
            "           Linear-16                 [-1, 1928]      24,680,328\n",
            "             ReLU-17                 [-1, 1928]               0\n",
            "          Dropout-18                 [-1, 1928]               0\n",
            "           Linear-19                  [-1, 737]       1,421,673\n",
            "             ReLU-20                  [-1, 737]               0\n",
            "          Dropout-21                  [-1, 737]               0\n",
            "           Linear-22                  [-1, 531]         391,878\n",
            "             ReLU-23                  [-1, 531]               0\n",
            "          Dropout-24                  [-1, 531]               0\n",
            "           Linear-25                  [-1, 406]         215,992\n",
            "             ReLU-26                  [-1, 406]               0\n",
            "          Dropout-27                  [-1, 406]               0\n",
            "           Linear-28                   [-1, 28]          11,396\n",
            "================================================================\n",
            "Total params: 26,772,851\n",
            "Trainable params: 26,772,851\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 42.79\n",
            "Params size (MB): 102.13\n",
            "Estimated Total Size (MB): 144.99\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.330691  [  256/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 09:29:24,884] Trial 5 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 5.7%, Val - loss: 3.278267;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 6               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5637760120602513, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(44, 118, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.47599320567161324, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(118, 85, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.13808268814036156, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(85, 73, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.6028041949315457, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=4672, out_features=792, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.1133831858369155, inplace=False)\n",
            "    (3): Linear(in_features=792, out_features=568, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.08439832874189174, inplace=False)\n",
            "    (6): Linear(in_features=568, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 44, 128, 128]             440\n",
            "              ReLU-2         [-1, 44, 128, 128]               0\n",
            "           Dropout-3         [-1, 44, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 44, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 44, 64, 64]               0\n",
            "            Conv2d-6          [-1, 118, 64, 64]          46,846\n",
            "              ReLU-7          [-1, 118, 64, 64]               0\n",
            "           Dropout-8          [-1, 118, 64, 64]               0\n",
            "         MaxPool2d-9          [-1, 118, 32, 32]               0\n",
            "        ConvBlock-10          [-1, 118, 32, 32]               0\n",
            "           Conv2d-11           [-1, 85, 32, 32]          90,355\n",
            "             ReLU-12           [-1, 85, 32, 32]               0\n",
            "          Dropout-13           [-1, 85, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 85, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 85, 16, 16]               0\n",
            "           Conv2d-16           [-1, 73, 16, 16]          55,918\n",
            "             ReLU-17           [-1, 73, 16, 16]               0\n",
            "          Dropout-18           [-1, 73, 16, 16]               0\n",
            "        MaxPool2d-19             [-1, 73, 8, 8]               0\n",
            "        ConvBlock-20             [-1, 73, 8, 8]               0\n",
            "           Linear-21                  [-1, 792]       3,701,016\n",
            "             ReLU-22                  [-1, 792]               0\n",
            "          Dropout-23                  [-1, 792]               0\n",
            "           Linear-24                  [-1, 568]         450,424\n",
            "             ReLU-25                  [-1, 568]               0\n",
            "          Dropout-26                  [-1, 568]               0\n",
            "           Linear-27                   [-1, 28]          15,932\n",
            "================================================================\n",
            "Total params: 4,360,931\n",
            "Trainable params: 4,360,931\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 35.01\n",
            "Params size (MB): 16.64\n",
            "Estimated Total Size (MB): 51.71\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.330834  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 22.8%, Val - loss: 3.195843;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=2 ------------------------------------------\n",
            "\n",
            "Training loss: 2.650316  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 41.8%, Val - loss: 3.013597;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=3 ------------------------------------------\n",
            "\n",
            "Training loss: 2.091465  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 49.4%, Val - loss: 2.841191;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=4 ------------------------------------------\n",
            "\n",
            "Training loss: 1.718232  [  256/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 09:31:26,169] Trial 6 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 58.9%, Val - loss: 2.704316;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 7               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.23285374063854802, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(125, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.3627805045170425, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(57, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.08076907388718083, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(59, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.3456772893928156, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (4): ConvBlock(\n",
            "      (conv): Conv2d(125, 105, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.09091636287769793, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=1680, out_features=134, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.14185686559157004, inplace=False)\n",
            "    (3): Linear(in_features=134, out_features=1826, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.3180588187411917, inplace=False)\n",
            "    (6): Linear(in_features=1826, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 125, 128, 128]           1,250\n",
            "              ReLU-2        [-1, 125, 128, 128]               0\n",
            "           Dropout-3        [-1, 125, 128, 128]               0\n",
            "         MaxPool2d-4          [-1, 125, 64, 64]               0\n",
            "         ConvBlock-5          [-1, 125, 64, 64]               0\n",
            "            Conv2d-6           [-1, 57, 64, 64]          64,182\n",
            "              ReLU-7           [-1, 57, 64, 64]               0\n",
            "           Dropout-8           [-1, 57, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 57, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 57, 32, 32]               0\n",
            "           Conv2d-11           [-1, 59, 32, 32]          30,326\n",
            "             ReLU-12           [-1, 59, 32, 32]               0\n",
            "          Dropout-13           [-1, 59, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 59, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 59, 16, 16]               0\n",
            "           Conv2d-16          [-1, 125, 16, 16]          66,500\n",
            "             ReLU-17          [-1, 125, 16, 16]               0\n",
            "          Dropout-18          [-1, 125, 16, 16]               0\n",
            "        MaxPool2d-19            [-1, 125, 8, 8]               0\n",
            "        ConvBlock-20            [-1, 125, 8, 8]               0\n",
            "           Conv2d-21            [-1, 105, 8, 8]         118,230\n",
            "             ReLU-22            [-1, 105, 8, 8]               0\n",
            "          Dropout-23            [-1, 105, 8, 8]               0\n",
            "        MaxPool2d-24            [-1, 105, 4, 4]               0\n",
            "        ConvBlock-25            [-1, 105, 4, 4]               0\n",
            "           Linear-26                  [-1, 134]         225,254\n",
            "             ReLU-27                  [-1, 134]               0\n",
            "          Dropout-28                  [-1, 134]               0\n",
            "           Linear-29                 [-1, 1826]         246,510\n",
            "             ReLU-30                 [-1, 1826]               0\n",
            "          Dropout-31                 [-1, 1826]               0\n",
            "           Linear-32                   [-1, 28]          51,156\n",
            "================================================================\n",
            "Total params: 803,408\n",
            "Trainable params: 803,408\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 63.61\n",
            "Params size (MB): 3.06\n",
            "Estimated Total Size (MB): 66.74\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.330620  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 15.2%, Val - loss: 3.063107;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=2 ------------------------------------------\n",
            "\n",
            "Training loss: 2.800532  [  256/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 27.9%, Val - loss: 2.937607;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=3 ------------------------------------------\n",
            "\n",
            "Training loss: 2.568394  [  256/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 09:32:56,702] Trial 7 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 28.6%, Val - loss: 2.771952;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 8               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 74, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5573764379099058, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(74, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.4117556190142945, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(45, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.3681264124420559, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=10752, out_features=133, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.022940916846032486, inplace=False)\n",
            "    (3): Linear(in_features=133, out_features=647, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.38798470084581516, inplace=False)\n",
            "    (6): Linear(in_features=647, out_features=1130, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.6572215349690781, inplace=False)\n",
            "    (9): Linear(in_features=1130, out_features=420, bias=True)\n",
            "    (10): ReLU()\n",
            "    (11): Dropout(p=0.6161229210733844, inplace=False)\n",
            "    (12): Linear(in_features=420, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 74, 128, 128]             740\n",
            "              ReLU-2         [-1, 74, 128, 128]               0\n",
            "           Dropout-3         [-1, 74, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 74, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 74, 64, 64]               0\n",
            "            Conv2d-6           [-1, 45, 64, 64]          30,015\n",
            "              ReLU-7           [-1, 45, 64, 64]               0\n",
            "           Dropout-8           [-1, 45, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 45, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 45, 32, 32]               0\n",
            "           Conv2d-11           [-1, 42, 32, 32]          17,052\n",
            "             ReLU-12           [-1, 42, 32, 32]               0\n",
            "          Dropout-13           [-1, 42, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 42, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 42, 16, 16]               0\n",
            "           Linear-16                  [-1, 133]       1,430,149\n",
            "             ReLU-17                  [-1, 133]               0\n",
            "          Dropout-18                  [-1, 133]               0\n",
            "           Linear-19                  [-1, 647]          86,698\n",
            "             ReLU-20                  [-1, 647]               0\n",
            "          Dropout-21                  [-1, 647]               0\n",
            "           Linear-22                 [-1, 1130]         732,240\n",
            "             ReLU-23                 [-1, 1130]               0\n",
            "          Dropout-24                 [-1, 1130]               0\n",
            "           Linear-25                  [-1, 420]         475,020\n",
            "             ReLU-26                  [-1, 420]               0\n",
            "          Dropout-27                  [-1, 420]               0\n",
            "           Linear-28                   [-1, 28]          11,788\n",
            "================================================================\n",
            "Total params: 2,783,702\n",
            "Trainable params: 2,783,702\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 38.50\n",
            "Params size (MB): 10.62\n",
            "Estimated Total Size (MB): 49.18\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.323147  [   32/ 8582]\n",
            "Training loss: 3.333670  [ 1632/ 8582]\n",
            "Training loss: 3.333723  [ 3232/ 8582]\n",
            "Training loss: 3.334403  [ 4832/ 8582]\n",
            "Training loss: 3.333636  [ 6432/ 8582]\n",
            "Training loss: 3.331783  [ 8032/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 09:33:28,172] Trial 8 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 4.0%, Val - loss: 3.332895;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 9               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 119, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.2932704989334546, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(119, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.6631974972895731, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=118784, out_features=1455, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.3368526910677814, inplace=False)\n",
            "    (3): Linear(in_features=1455, out_features=1901, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.6339844324018805, inplace=False)\n",
            "    (6): Linear(in_features=1901, out_features=1172, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.4585352352143173, inplace=False)\n",
            "    (9): Linear(in_features=1172, out_features=235, bias=True)\n",
            "    (10): ReLU()\n",
            "    (11): Dropout(p=0.3951973901177386, inplace=False)\n",
            "    (12): Linear(in_features=235, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 119, 128, 128]           1,190\n",
            "              ReLU-2        [-1, 119, 128, 128]               0\n",
            "           Dropout-3        [-1, 119, 128, 128]               0\n",
            "         MaxPool2d-4          [-1, 119, 64, 64]               0\n",
            "         ConvBlock-5          [-1, 119, 64, 64]               0\n",
            "            Conv2d-6          [-1, 116, 64, 64]         124,352\n",
            "              ReLU-7          [-1, 116, 64, 64]               0\n",
            "           Dropout-8          [-1, 116, 64, 64]               0\n",
            "         MaxPool2d-9          [-1, 116, 32, 32]               0\n",
            "        ConvBlock-10          [-1, 116, 32, 32]               0\n",
            "           Linear-11                 [-1, 1455]     172,832,175\n",
            "             ReLU-12                 [-1, 1455]               0\n",
            "          Dropout-13                 [-1, 1455]               0\n",
            "           Linear-14                 [-1, 1901]       2,767,856\n",
            "             ReLU-15                 [-1, 1901]               0\n",
            "          Dropout-16                 [-1, 1901]               0\n",
            "           Linear-17                 [-1, 1172]       2,229,144\n",
            "             ReLU-18                 [-1, 1172]               0\n",
            "          Dropout-19                 [-1, 1172]               0\n",
            "           Linear-20                  [-1, 235]         275,655\n",
            "             ReLU-21                  [-1, 235]               0\n",
            "          Dropout-22                  [-1, 235]               0\n",
            "           Linear-23                   [-1, 28]           6,608\n",
            "================================================================\n",
            "Total params: 178,236,980\n",
            "Trainable params: 178,236,980\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 64.86\n",
            "Params size (MB): 679.92\n",
            "Estimated Total Size (MB): 744.84\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.331739  [  256/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 09:34:01,865] Trial 9 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 2.7%, Val - loss: 3.331166;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 10               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.4058869148198627, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(61, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.19798127684221564, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(63, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.6954214721521508, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(128, 111, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.053217790091843686, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (4): ConvBlock(\n",
            "      (conv): Conv2d(111, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.6920525383004958, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=736, out_features=589, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.44832368848644916, inplace=False)\n",
            "    (3): Linear(in_features=589, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 61, 128, 128]             610\n",
            "              ReLU-2         [-1, 61, 128, 128]               0\n",
            "           Dropout-3         [-1, 61, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 61, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 61, 64, 64]               0\n",
            "            Conv2d-6           [-1, 63, 64, 64]          34,650\n",
            "              ReLU-7           [-1, 63, 64, 64]               0\n",
            "           Dropout-8           [-1, 63, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 63, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 63, 32, 32]               0\n",
            "           Conv2d-11          [-1, 128, 32, 32]          72,704\n",
            "             ReLU-12          [-1, 128, 32, 32]               0\n",
            "          Dropout-13          [-1, 128, 32, 32]               0\n",
            "        MaxPool2d-14          [-1, 128, 16, 16]               0\n",
            "        ConvBlock-15          [-1, 128, 16, 16]               0\n",
            "           Conv2d-16          [-1, 111, 16, 16]         127,983\n",
            "             ReLU-17          [-1, 111, 16, 16]               0\n",
            "          Dropout-18          [-1, 111, 16, 16]               0\n",
            "        MaxPool2d-19            [-1, 111, 8, 8]               0\n",
            "        ConvBlock-20            [-1, 111, 8, 8]               0\n",
            "           Conv2d-21             [-1, 46, 8, 8]          46,000\n",
            "             ReLU-22             [-1, 46, 8, 8]               0\n",
            "          Dropout-23             [-1, 46, 8, 8]               0\n",
            "        MaxPool2d-24             [-1, 46, 4, 4]               0\n",
            "        ConvBlock-25             [-1, 46, 4, 4]               0\n",
            "           Linear-26                  [-1, 589]         434,093\n",
            "             ReLU-27                  [-1, 589]               0\n",
            "          Dropout-28                  [-1, 589]               0\n",
            "           Linear-29                   [-1, 28]          16,520\n",
            "================================================================\n",
            "Total params: 732,560\n",
            "Trainable params: 732,560\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 37.93\n",
            "Params size (MB): 2.79\n",
            "Estimated Total Size (MB): 40.79\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.318893  [   32/ 8582]\n",
            "Training loss: 3.254992  [ 1632/ 8582]\n",
            "Training loss: 3.062998  [ 3232/ 8582]\n",
            "Training loss: 3.021005  [ 4832/ 8582]\n",
            "Training loss: 2.919222  [ 6432/ 8582]\n",
            "Training loss: 2.904076  [ 8032/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 09:34:32,891] Trial 10 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 12.6%, Val - loss: 3.160440;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 11               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 101, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.17952576843808826, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(101, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5584144432232029, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(97, 77, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5303991676768367, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(77, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.00048444490025634224, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=6144, out_features=509, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2716904048298922, inplace=False)\n",
            "    (3): Linear(in_features=509, out_features=1243, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.007404766667070822, inplace=False)\n",
            "    (6): Linear(in_features=1243, out_features=140, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.45882682472418224, inplace=False)\n",
            "    (9): Linear(in_features=140, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 101, 128, 128]           1,010\n",
            "              ReLU-2        [-1, 101, 128, 128]               0\n",
            "           Dropout-3        [-1, 101, 128, 128]               0\n",
            "         MaxPool2d-4          [-1, 101, 64, 64]               0\n",
            "         ConvBlock-5          [-1, 101, 64, 64]               0\n",
            "            Conv2d-6           [-1, 97, 64, 64]          88,270\n",
            "              ReLU-7           [-1, 97, 64, 64]               0\n",
            "           Dropout-8           [-1, 97, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 97, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 97, 32, 32]               0\n",
            "           Conv2d-11           [-1, 77, 32, 32]          67,298\n",
            "             ReLU-12           [-1, 77, 32, 32]               0\n",
            "          Dropout-13           [-1, 77, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 77, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 77, 16, 16]               0\n",
            "           Conv2d-16           [-1, 96, 16, 16]          66,624\n",
            "             ReLU-17           [-1, 96, 16, 16]               0\n",
            "          Dropout-18           [-1, 96, 16, 16]               0\n",
            "        MaxPool2d-19             [-1, 96, 8, 8]               0\n",
            "        ConvBlock-20             [-1, 96, 8, 8]               0\n",
            "           Linear-21                  [-1, 509]       3,127,805\n",
            "             ReLU-22                  [-1, 509]               0\n",
            "          Dropout-23                  [-1, 509]               0\n",
            "           Linear-24                 [-1, 1243]         633,930\n",
            "             ReLU-25                 [-1, 1243]               0\n",
            "          Dropout-26                 [-1, 1243]               0\n",
            "           Linear-27                  [-1, 140]         174,160\n",
            "             ReLU-28                  [-1, 140]               0\n",
            "          Dropout-29                  [-1, 140]               0\n",
            "           Linear-30                   [-1, 28]           3,948\n",
            "================================================================\n",
            "Total params: 4,163,045\n",
            "Trainable params: 4,163,045\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 57.60\n",
            "Params size (MB): 15.88\n",
            "Estimated Total Size (MB): 73.55\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.326361  [  128/ 8582]\n",
            "Training loss: 3.087448  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 18.1%, Val - loss: 3.092546;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=2 ------------------------------------------\n",
            "\n",
            "Training loss: 2.830299  [  128/ 8582]\n",
            "Training loss: 2.720168  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 33.3%, Val - loss: 2.810920;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=3 ------------------------------------------\n",
            "\n",
            "Training loss: 2.493878  [  128/ 8582]\n",
            "Training loss: 2.134943  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 55.6%, Val - loss: 2.269071;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=4 ------------------------------------------\n",
            "\n",
            "Training loss: 1.825571  [  128/ 8582]\n",
            "Training loss: 1.427598  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 76.2%, Val - loss: 1.747879;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=5 ------------------------------------------\n",
            "\n",
            "Training loss: 1.160612  [  128/ 8582]\n",
            "Training loss: 0.906546  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 82.1%, Val - loss: 1.392761;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=6 ------------------------------------------\n",
            "\n",
            "Training loss: 0.881247  [  128/ 8582]\n",
            "Training loss: 0.675371  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 86.5%, Val - loss: 1.066563;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=7 ------------------------------------------\n",
            "\n",
            "Training loss: 0.789934  [  128/ 8582]\n",
            "Training loss: 0.536994  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 87.7%, Val - loss: 1.033913;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=8 ------------------------------------------\n",
            "\n",
            "Training loss: 0.742871  [  128/ 8582]\n",
            "Training loss: 0.435369  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 89.9%, Val - loss: 0.895461;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=9 ------------------------------------------\n",
            "\n",
            "Training loss: 0.415332  [  128/ 8582]\n",
            "Training loss: 0.372896  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.3%, Val - loss: 0.741410;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=10 ------------------------------------------\n",
            "\n",
            "Training loss: 0.287896  [  128/ 8582]\n",
            "Training loss: 0.331555  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.6%, Val - loss: 0.780528;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=11 ------------------------------------------\n",
            "\n",
            "Training loss: 0.368868  [  128/ 8582]\n",
            "Training loss: 0.277096  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.6%, Val - loss: 0.596937;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=12 ------------------------------------------\n",
            "\n",
            "Training loss: 0.243836  [  128/ 8582]\n",
            "Training loss: 0.249457  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.7%, Val - loss: 0.586130;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=13 ------------------------------------------\n",
            "\n",
            "Training loss: 0.249640  [  128/ 8582]\n",
            "Training loss: 0.221357  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.3%, Val - loss: 0.584949;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=14 ------------------------------------------\n",
            "\n",
            "Training loss: 0.222861  [  128/ 8582]\n",
            "Training loss: 0.203806  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.1%, Val - loss: 0.529116;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=15 ------------------------------------------\n",
            "\n",
            "Training loss: 0.128004  [  128/ 8582]\n",
            "Training loss: 0.195460  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.4%, Val - loss: 0.489025;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=16 ------------------------------------------\n",
            "\n",
            "Training loss: 0.106634  [  128/ 8582]\n",
            "Training loss: 0.169015  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.1%, Val - loss: 0.511198;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=17 ------------------------------------------\n",
            "\n",
            "Training loss: 0.228421  [  128/ 8582]\n",
            "Training loss: 0.178805  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.1%, Val - loss: 0.525093;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=18 ------------------------------------------\n",
            "\n",
            "Training loss: 0.081694  [  128/ 8582]\n",
            "Training loss: 0.144361  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.2%, Val - loss: 0.558179;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=19 ------------------------------------------\n",
            "\n",
            "Training loss: 0.097491  [  128/ 8582]\n",
            "Training loss: 0.155077  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.0%, Val - loss: 0.412520;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=20 ------------------------------------------\n",
            "\n",
            "Training loss: 0.037738  [  128/ 8582]\n",
            "Training loss: 0.145445  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.0%, Val - loss: 0.449322;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=21 ------------------------------------------\n",
            "\n",
            "Training loss: 0.152683  [  128/ 8582]\n",
            "Training loss: 0.134210  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.2%, Val - loss: 0.475055;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=22 ------------------------------------------\n",
            "\n",
            "Training loss: 0.171270  [  128/ 8582]\n",
            "Training loss: 0.137111  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.0%, Val - loss: 0.466309;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=23 ------------------------------------------\n",
            "\n",
            "Training loss: 0.054960  [  128/ 8582]\n",
            "Training loss: 0.122109  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.3%, Val - loss: 0.394393;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=24 ------------------------------------------\n",
            "\n",
            "Training loss: 0.132216  [  128/ 8582]\n",
            "Training loss: 0.117625  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.5%, Val - loss: 0.408993;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=25 ------------------------------------------\n",
            "\n",
            "Training loss: 0.049041  [  128/ 8582]\n",
            "Training loss: 0.116979  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.2%, Val - loss: 0.489178;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=26 ------------------------------------------\n",
            "\n",
            "Training loss: 0.093139  [  128/ 8582]\n",
            "Training loss: 0.095784  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.8%, Val - loss: 0.442977;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=27 ------------------------------------------\n",
            "\n",
            "Training loss: 0.193869  [  128/ 8582]\n",
            "Training loss: 0.096499  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.0%, Val - loss: 0.413337;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=28 ------------------------------------------\n",
            "\n",
            "Training loss: 0.105740  [  128/ 8582]\n",
            "Training loss: 0.096810  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.5%, Val - loss: 0.397298;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=29 ------------------------------------------\n",
            "\n",
            "Training loss: 0.053912  [  128/ 8582]\n",
            "Training loss: 0.094953  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.9%, Val - loss: 0.328060;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=30 ------------------------------------------\n",
            "\n",
            "Training loss: 0.128501  [  128/ 8582]\n",
            "Training loss: 0.078108  [ 6528/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 09:48:45,674] Trial 11 finished with value: 0.9293094072865687 and parameters: {'nr_conv_blocks': 4, 'out_channels_0': 101, 'out_channels_1': 97, 'out_channels_2': 77, 'out_channels_3': 96, 'dropout_rate_0': 0.17952576843808826, 'dropout_rate_1': 0.5584144432232029, 'dropout_rate_2': 0.5303991676768367, 'dropout_rate_3': 0.00048444490025634224, 'nr_fc_layers': 3, 'fc_size_0': 509, 'fc_size_1': 1243, 'fc_size_2': 140, 'dropout_rate_fc_0': 0.2716904048298922, 'dropout_rate_fc_1': 0.007404766667070822, 'dropout_rate_fc_2': 0.45882682472418224, 'optimizer': 'Adam', 'lr': 0.0005370239124727673, 'batch_size': 128}. Best is trial 3 with value: 0.9467101685698749.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 92.9%, Val - loss: 0.369171;\n",
            "\n",
            "#############################################\n",
            "#               Finished trial - 11/30               #\n",
            "#############################################\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 12               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 94, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.20224897767426922, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(94, 70, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5786110883067361, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(70, 83, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.2502551250519498, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(83, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.00035066694709973145, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=5888, out_features=423, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.26232000207487205, inplace=False)\n",
            "    (3): Linear(in_features=423, out_features=170, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.018109827130118106, inplace=False)\n",
            "    (6): Linear(in_features=170, out_features=1965, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.33732024656987025, inplace=False)\n",
            "    (9): Linear(in_features=1965, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 94, 128, 128]             940\n",
            "              ReLU-2         [-1, 94, 128, 128]               0\n",
            "           Dropout-3         [-1, 94, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 94, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 94, 64, 64]               0\n",
            "            Conv2d-6           [-1, 70, 64, 64]          59,290\n",
            "              ReLU-7           [-1, 70, 64, 64]               0\n",
            "           Dropout-8           [-1, 70, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 70, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 70, 32, 32]               0\n",
            "           Conv2d-11           [-1, 83, 32, 32]          52,373\n",
            "             ReLU-12           [-1, 83, 32, 32]               0\n",
            "          Dropout-13           [-1, 83, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 83, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 83, 16, 16]               0\n",
            "           Conv2d-16           [-1, 92, 16, 16]          68,816\n",
            "             ReLU-17           [-1, 92, 16, 16]               0\n",
            "          Dropout-18           [-1, 92, 16, 16]               0\n",
            "        MaxPool2d-19             [-1, 92, 8, 8]               0\n",
            "        ConvBlock-20             [-1, 92, 8, 8]               0\n",
            "           Linear-21                  [-1, 423]       2,491,047\n",
            "             ReLU-22                  [-1, 423]               0\n",
            "          Dropout-23                  [-1, 423]               0\n",
            "           Linear-24                  [-1, 170]          72,080\n",
            "             ReLU-25                  [-1, 170]               0\n",
            "          Dropout-26                  [-1, 170]               0\n",
            "           Linear-27                 [-1, 1965]         336,015\n",
            "             ReLU-28                 [-1, 1965]               0\n",
            "          Dropout-29                 [-1, 1965]               0\n",
            "           Linear-30                   [-1, 28]          55,048\n",
            "================================================================\n",
            "Total params: 3,135,609\n",
            "Trainable params: 3,135,609\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 51.74\n",
            "Params size (MB): 11.96\n",
            "Estimated Total Size (MB): 63.76\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.330611  [  128/ 8582]\n",
            "Training loss: 2.968953  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 22.1%, Val - loss: 2.843081;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=2 ------------------------------------------\n",
            "\n",
            "Training loss: 2.617727  [  128/ 8582]\n",
            "Training loss: 2.367062  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 41.7%, Val - loss: 2.261801;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=3 ------------------------------------------\n",
            "\n",
            "Training loss: 1.680583  [  128/ 8582]\n",
            "Training loss: 1.381217  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 68.7%, Val - loss: 1.573764;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=4 ------------------------------------------\n",
            "\n",
            "Training loss: 0.879809  [  128/ 8582]\n",
            "Training loss: 0.813282  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 81.3%, Val - loss: 1.041822;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=5 ------------------------------------------\n",
            "\n",
            "Training loss: 0.504058  [  128/ 8582]\n",
            "Training loss: 0.534991  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 85.8%, Val - loss: 0.845890;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=6 ------------------------------------------\n",
            "\n",
            "Training loss: 0.297998  [  128/ 8582]\n",
            "Training loss: 0.417815  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 86.5%, Val - loss: 0.747906;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=7 ------------------------------------------\n",
            "\n",
            "Training loss: 0.202595  [  128/ 8582]\n",
            "Training loss: 0.324094  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 88.5%, Val - loss: 0.677872;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=8 ------------------------------------------\n",
            "\n",
            "Training loss: 0.333020  [  128/ 8582]\n",
            "Training loss: 0.274203  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.0%, Val - loss: 0.588656;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=9 ------------------------------------------\n",
            "\n",
            "Training loss: 0.189563  [  128/ 8582]\n",
            "Training loss: 0.247559  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 89.1%, Val - loss: 0.526944;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=10 ------------------------------------------\n",
            "\n",
            "Training loss: 0.125450  [  128/ 8582]\n",
            "Training loss: 0.215096  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 89.5%, Val - loss: 0.510633;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=11 ------------------------------------------\n",
            "\n",
            "Training loss: 0.169583  [  128/ 8582]\n",
            "Training loss: 0.183188  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.5%, Val - loss: 0.443353;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=12 ------------------------------------------\n",
            "\n",
            "Training loss: 0.095327  [  128/ 8582]\n",
            "Training loss: 0.171042  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 88.6%, Val - loss: 0.525466;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=13 ------------------------------------------\n",
            "\n",
            "Training loss: 0.302015  [  128/ 8582]\n",
            "Training loss: 0.158091  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.3%, Val - loss: 0.469512;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=14 ------------------------------------------\n",
            "\n",
            "Training loss: 0.140988  [  128/ 8582]\n",
            "Training loss: 0.151079  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.1%, Val - loss: 0.408825;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=15 ------------------------------------------\n",
            "\n",
            "Training loss: 0.100279  [  128/ 8582]\n",
            "Training loss: 0.127502  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.1%, Val - loss: 0.408151;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=16 ------------------------------------------\n",
            "\n",
            "Training loss: 0.083764  [  128/ 8582]\n",
            "Training loss: 0.114551  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.6%, Val - loss: 0.408303;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=17 ------------------------------------------\n",
            "\n",
            "Training loss: 0.143825  [  128/ 8582]\n",
            "Training loss: 0.126877  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.8%, Val - loss: 0.400444;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=18 ------------------------------------------\n",
            "\n",
            "Training loss: 0.117407  [  128/ 8582]\n",
            "Training loss: 0.119997  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.2%, Val - loss: 0.391377;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=19 ------------------------------------------\n",
            "\n",
            "Training loss: 0.047196  [  128/ 8582]\n",
            "Training loss: 0.098755  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.1%, Val - loss: 0.386387;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=20 ------------------------------------------\n",
            "\n",
            "Training loss: 0.067691  [  128/ 8582]\n",
            "Training loss: 0.090401  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.8%, Val - loss: 0.358217;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=21 ------------------------------------------\n",
            "\n",
            "Training loss: 0.087388  [  128/ 8582]\n",
            "Training loss: 0.087350  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.1%, Val - loss: 0.413042;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=22 ------------------------------------------\n",
            "\n",
            "Training loss: 0.078913  [  128/ 8582]\n",
            "Training loss: 0.085022  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.8%, Val - loss: 0.329657;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=23 ------------------------------------------\n",
            "\n",
            "Training loss: 0.015747  [  128/ 8582]\n",
            "Training loss: 0.097206  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.5%, Val - loss: 0.324556;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=24 ------------------------------------------\n",
            "\n",
            "Training loss: 0.111229  [  128/ 8582]\n",
            "Training loss: 0.072984  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.7%, Val - loss: 0.285247;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=25 ------------------------------------------\n",
            "\n",
            "Training loss: 0.048584  [  128/ 8582]\n",
            "Training loss: 0.064554  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.2%, Val - loss: 0.326507;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=26 ------------------------------------------\n",
            "\n",
            "Training loss: 0.017701  [  128/ 8582]\n",
            "Training loss: 0.070206  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.4%, Val - loss: 0.295136;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=27 ------------------------------------------\n",
            "\n",
            "Training loss: 0.134304  [  128/ 8582]\n",
            "Training loss: 0.075693  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.1%, Val - loss: 0.392369;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=28 ------------------------------------------\n",
            "\n",
            "Training loss: 0.078748  [  128/ 8582]\n",
            "Training loss: 0.069237  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.0%, Val - loss: 0.274875;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=29 ------------------------------------------\n",
            "\n",
            "Training loss: 0.009571  [  128/ 8582]\n",
            "Training loss: 0.081047  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.2%, Val - loss: 0.389097;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=30 ------------------------------------------\n",
            "\n",
            "Training loss: 0.242561  [  128/ 8582]\n",
            "Training loss: 0.169487  [ 6528/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:02:29,174] Trial 12 finished with value: 0.9358346927678086 and parameters: {'nr_conv_blocks': 4, 'out_channels_0': 94, 'out_channels_1': 70, 'out_channels_2': 83, 'out_channels_3': 92, 'dropout_rate_0': 0.20224897767426922, 'dropout_rate_1': 0.5786110883067361, 'dropout_rate_2': 0.2502551250519498, 'dropout_rate_3': 0.00035066694709973145, 'nr_fc_layers': 3, 'fc_size_0': 423, 'fc_size_1': 170, 'fc_size_2': 1965, 'dropout_rate_fc_0': 0.26232000207487205, 'dropout_rate_fc_1': 0.018109827130118106, 'dropout_rate_fc_2': 0.33732024656987025, 'optimizer': 'Adam', 'lr': 0.0008680001292706698, 'batch_size': 128}. Best is trial 3 with value: 0.9467101685698749.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 93.6%, Val - loss: 0.371325;\n",
            "\n",
            "#############################################\n",
            "#               Finished trial - 12/30               #\n",
            "#############################################\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 13               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.18038260975152948, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(86, 68, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5604257341227041, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(68, 87, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.23509269974096164, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(87, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.18467456937318183, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=5504, out_features=490, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.27388122895142153, inplace=False)\n",
            "    (3): Linear(in_features=490, out_features=128, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.18102426882308317, inplace=False)\n",
            "    (6): Linear(in_features=128, out_features=1887, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.25421451479105833, inplace=False)\n",
            "    (9): Linear(in_features=1887, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 86, 128, 128]             860\n",
            "              ReLU-2         [-1, 86, 128, 128]               0\n",
            "           Dropout-3         [-1, 86, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 86, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 86, 64, 64]               0\n",
            "            Conv2d-6           [-1, 68, 64, 64]          52,700\n",
            "              ReLU-7           [-1, 68, 64, 64]               0\n",
            "           Dropout-8           [-1, 68, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 68, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 68, 32, 32]               0\n",
            "           Conv2d-11           [-1, 87, 32, 32]          53,331\n",
            "             ReLU-12           [-1, 87, 32, 32]               0\n",
            "          Dropout-13           [-1, 87, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 87, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 87, 16, 16]               0\n",
            "           Conv2d-16           [-1, 86, 16, 16]          67,424\n",
            "             ReLU-17           [-1, 86, 16, 16]               0\n",
            "          Dropout-18           [-1, 86, 16, 16]               0\n",
            "        MaxPool2d-19             [-1, 86, 8, 8]               0\n",
            "        ConvBlock-20             [-1, 86, 8, 8]               0\n",
            "           Linear-21                  [-1, 490]       2,697,450\n",
            "             ReLU-22                  [-1, 490]               0\n",
            "          Dropout-23                  [-1, 490]               0\n",
            "           Linear-24                  [-1, 128]          62,848\n",
            "             ReLU-25                  [-1, 128]               0\n",
            "          Dropout-26                  [-1, 128]               0\n",
            "           Linear-27                 [-1, 1887]         243,423\n",
            "             ReLU-28                 [-1, 1887]               0\n",
            "          Dropout-29                 [-1, 1887]               0\n",
            "           Linear-30                   [-1, 28]          52,864\n",
            "================================================================\n",
            "Total params: 3,230,900\n",
            "Trainable params: 3,230,900\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 48.09\n",
            "Params size (MB): 12.32\n",
            "Estimated Total Size (MB): 60.47\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.330946  [  128/ 8582]\n",
            "Training loss: 3.332414  [ 6528/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:02:57,967] Trial 13 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 3.8%, Val - loss: 3.332866;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 14               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 91, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.6782195935986226, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(91, 73, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.008771719492022767, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(73, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.27322596076651173, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(72, 70, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.027786015527953567, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (4): ConvBlock(\n",
            "      (conv): Conv2d(70, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.08400479443495024, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=2032, out_features=345, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.4629415432006383, inplace=False)\n",
            "    (3): Linear(in_features=345, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 91, 128, 128]             910\n",
            "              ReLU-2         [-1, 91, 128, 128]               0\n",
            "           Dropout-3         [-1, 91, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 91, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 91, 64, 64]               0\n",
            "            Conv2d-6           [-1, 73, 64, 64]          59,860\n",
            "              ReLU-7           [-1, 73, 64, 64]               0\n",
            "           Dropout-8           [-1, 73, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 73, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 73, 32, 32]               0\n",
            "           Conv2d-11           [-1, 72, 32, 32]          47,376\n",
            "             ReLU-12           [-1, 72, 32, 32]               0\n",
            "          Dropout-13           [-1, 72, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 72, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 72, 16, 16]               0\n",
            "           Conv2d-16           [-1, 70, 16, 16]          45,430\n",
            "             ReLU-17           [-1, 70, 16, 16]               0\n",
            "          Dropout-18           [-1, 70, 16, 16]               0\n",
            "        MaxPool2d-19             [-1, 70, 8, 8]               0\n",
            "        ConvBlock-20             [-1, 70, 8, 8]               0\n",
            "           Conv2d-21            [-1, 127, 8, 8]          80,137\n",
            "             ReLU-22            [-1, 127, 8, 8]               0\n",
            "          Dropout-23            [-1, 127, 8, 8]               0\n",
            "        MaxPool2d-24            [-1, 127, 4, 4]               0\n",
            "        ConvBlock-25            [-1, 127, 4, 4]               0\n",
            "           Linear-26                  [-1, 345]         701,385\n",
            "             ReLU-27                  [-1, 345]               0\n",
            "          Dropout-28                  [-1, 345]               0\n",
            "           Linear-29                   [-1, 28]           9,688\n",
            "================================================================\n",
            "Total params: 944,786\n",
            "Trainable params: 944,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 50.47\n",
            "Params size (MB): 3.60\n",
            "Estimated Total Size (MB): 54.14\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.341276  [   32/ 8582]\n",
            "Training loss: 3.087656  [ 1632/ 8582]\n",
            "Training loss: 2.888869  [ 3232/ 8582]\n",
            "Training loss: 2.745683  [ 4832/ 8582]\n",
            "Training loss: 2.596886  [ 6432/ 8582]\n",
            "Training loss: 2.421754  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 29.9%, Val - loss: 2.702091;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=2 ------------------------------------------\n",
            "\n",
            "Training loss: 2.368754  [   32/ 8582]\n",
            "Training loss: 2.265206  [ 1632/ 8582]\n",
            "Training loss: 2.088747  [ 3232/ 8582]\n",
            "Training loss: 1.909117  [ 4832/ 8582]\n",
            "Training loss: 1.673873  [ 6432/ 8582]\n",
            "Training loss: 1.541918  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 58.6%, Val - loss: 1.757629;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=3 ------------------------------------------\n",
            "\n",
            "Training loss: 1.125751  [   32/ 8582]\n",
            "Training loss: 1.326358  [ 1632/ 8582]\n",
            "Training loss: 1.257238  [ 3232/ 8582]\n",
            "Training loss: 1.171520  [ 4832/ 8582]\n",
            "Training loss: 0.971756  [ 6432/ 8582]\n",
            "Training loss: 1.015897  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 60.9%, Val - loss: 1.423392;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=4 ------------------------------------------\n",
            "\n",
            "Training loss: 0.913533  [   32/ 8582]\n",
            "Training loss: 0.829418  [ 1632/ 8582]\n",
            "Training loss: 0.852047  [ 3232/ 8582]\n",
            "Training loss: 0.798009  [ 4832/ 8582]\n",
            "Training loss: 0.692057  [ 6432/ 8582]\n",
            "Training loss: 0.726217  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 75.1%, Val - loss: 1.035180;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=5 ------------------------------------------\n",
            "\n",
            "Training loss: 0.536832  [   32/ 8582]\n",
            "Training loss: 0.609124  [ 1632/ 8582]\n",
            "Training loss: 0.585663  [ 3232/ 8582]\n",
            "Training loss: 0.546398  [ 4832/ 8582]\n",
            "Training loss: 0.542395  [ 6432/ 8582]\n",
            "Training loss: 0.565036  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 82.2%, Val - loss: 0.776895;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=6 ------------------------------------------\n",
            "\n",
            "Training loss: 0.463449  [   32/ 8582]\n",
            "Training loss: 0.535467  [ 1632/ 8582]\n",
            "Training loss: 0.533433  [ 3232/ 8582]\n",
            "Training loss: 0.530839  [ 4832/ 8582]\n",
            "Training loss: 0.423558  [ 6432/ 8582]\n",
            "Training loss: 0.440937  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 76.8%, Val - loss: 0.868191;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=7 ------------------------------------------\n",
            "\n",
            "Training loss: 0.451731  [   32/ 8582]\n",
            "Training loss: 0.424099  [ 1632/ 8582]\n",
            "Training loss: 0.409191  [ 3232/ 8582]\n",
            "Training loss: 0.428378  [ 4832/ 8582]\n",
            "Training loss: 0.403260  [ 6432/ 8582]\n",
            "Training loss: 0.418417  [ 8032/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:06:13,855] Trial 14 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 81.2%, Val - loss: 0.757757;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 15               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 111, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.2948948707219488, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(111, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.600530953258403, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(52, 99, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.2241155416290505, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(99, 101, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.1810632328821001, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=6464, out_features=783, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2204592678457129, inplace=False)\n",
            "    (3): Linear(in_features=783, out_features=352, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.016742067093336865, inplace=False)\n",
            "    (6): Linear(in_features=352, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 111, 128, 128]           1,110\n",
            "              ReLU-2        [-1, 111, 128, 128]               0\n",
            "           Dropout-3        [-1, 111, 128, 128]               0\n",
            "         MaxPool2d-4          [-1, 111, 64, 64]               0\n",
            "         ConvBlock-5          [-1, 111, 64, 64]               0\n",
            "            Conv2d-6           [-1, 52, 64, 64]          52,000\n",
            "              ReLU-7           [-1, 52, 64, 64]               0\n",
            "           Dropout-8           [-1, 52, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 52, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 52, 32, 32]               0\n",
            "           Conv2d-11           [-1, 99, 32, 32]          46,431\n",
            "             ReLU-12           [-1, 99, 32, 32]               0\n",
            "          Dropout-13           [-1, 99, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 99, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 99, 16, 16]               0\n",
            "           Conv2d-16          [-1, 101, 16, 16]          90,092\n",
            "             ReLU-17          [-1, 101, 16, 16]               0\n",
            "          Dropout-18          [-1, 101, 16, 16]               0\n",
            "        MaxPool2d-19            [-1, 101, 8, 8]               0\n",
            "        ConvBlock-20            [-1, 101, 8, 8]               0\n",
            "           Linear-21                  [-1, 783]       5,062,095\n",
            "             ReLU-22                  [-1, 783]               0\n",
            "          Dropout-23                  [-1, 783]               0\n",
            "           Linear-24                  [-1, 352]         275,968\n",
            "             ReLU-25                  [-1, 352]               0\n",
            "          Dropout-26                  [-1, 352]               0\n",
            "           Linear-27                   [-1, 28]           9,884\n",
            "================================================================\n",
            "Total params: 5,537,580\n",
            "Trainable params: 5,537,580\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 57.67\n",
            "Params size (MB): 21.12\n",
            "Estimated Total Size (MB): 78.86\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.333288  [   64/ 8582]\n",
            "Training loss: 5685771.744800  [ 3264/ 8582]\n",
            "Training loss: 8.215554  [ 6464/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 5.8%, Val - loss: 5.396720;\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:06:43,088] Trial 15 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 16               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.12846907079443204, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(51, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.49372488494187283, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(32, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.15609902934608827, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=25600, out_features=321, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.4171987702580672, inplace=False)\n",
            "    (3): Linear(in_features=321, out_features=988, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.20475985526249235, inplace=False)\n",
            "    (6): Linear(in_features=988, out_features=1989, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.05196232996830402, inplace=False)\n",
            "    (9): Linear(in_features=1989, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 51, 128, 128]             510\n",
            "              ReLU-2         [-1, 51, 128, 128]               0\n",
            "           Dropout-3         [-1, 51, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 51, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 51, 64, 64]               0\n",
            "            Conv2d-6           [-1, 32, 64, 64]          14,720\n",
            "              ReLU-7           [-1, 32, 64, 64]               0\n",
            "           Dropout-8           [-1, 32, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 32, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 32, 32, 32]               0\n",
            "           Conv2d-11          [-1, 100, 32, 32]          28,900\n",
            "             ReLU-12          [-1, 100, 32, 32]               0\n",
            "          Dropout-13          [-1, 100, 32, 32]               0\n",
            "        MaxPool2d-14          [-1, 100, 16, 16]               0\n",
            "        ConvBlock-15          [-1, 100, 16, 16]               0\n",
            "           Linear-16                  [-1, 321]       8,217,921\n",
            "             ReLU-17                  [-1, 321]               0\n",
            "          Dropout-18                  [-1, 321]               0\n",
            "           Linear-19                  [-1, 988]         318,136\n",
            "             ReLU-20                  [-1, 988]               0\n",
            "          Dropout-21                  [-1, 988]               0\n",
            "           Linear-22                 [-1, 1989]       1,967,121\n",
            "             ReLU-23                 [-1, 1989]               0\n",
            "          Dropout-24                 [-1, 1989]               0\n",
            "           Linear-25                   [-1, 28]          55,720\n",
            "================================================================\n",
            "Total params: 10,603,028\n",
            "Trainable params: 10,603,028\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 28.62\n",
            "Params size (MB): 40.45\n",
            "Estimated Total Size (MB): 69.13\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.332323  [  128/ 8582]\n",
            "Training loss: 3.332168  [ 6528/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:07:11,659] Trial 16 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 4.2%, Val - loss: 3.331937;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 17               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.2538738346251522, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(92, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.6755925345401765, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(80, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.310189727627883, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(66, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.12427760702123512, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (4): ConvBlock(\n",
            "      (conv): Conv2d(46, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.49208082114675544, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=560, out_features=759, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5232014518187657, inplace=False)\n",
            "    (3): Linear(in_features=759, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 92, 128, 128]             920\n",
            "              ReLU-2         [-1, 92, 128, 128]               0\n",
            "           Dropout-3         [-1, 92, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 92, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 92, 64, 64]               0\n",
            "            Conv2d-6           [-1, 80, 64, 64]          66,320\n",
            "              ReLU-7           [-1, 80, 64, 64]               0\n",
            "           Dropout-8           [-1, 80, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 80, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 80, 32, 32]               0\n",
            "           Conv2d-11           [-1, 66, 32, 32]          47,586\n",
            "             ReLU-12           [-1, 66, 32, 32]               0\n",
            "          Dropout-13           [-1, 66, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 66, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 66, 16, 16]               0\n",
            "           Conv2d-16           [-1, 46, 16, 16]          27,370\n",
            "             ReLU-17           [-1, 46, 16, 16]               0\n",
            "          Dropout-18           [-1, 46, 16, 16]               0\n",
            "        MaxPool2d-19             [-1, 46, 8, 8]               0\n",
            "        ConvBlock-20             [-1, 46, 8, 8]               0\n",
            "           Conv2d-21             [-1, 35, 8, 8]          14,525\n",
            "             ReLU-22             [-1, 35, 8, 8]               0\n",
            "          Dropout-23             [-1, 35, 8, 8]               0\n",
            "        MaxPool2d-24             [-1, 35, 4, 4]               0\n",
            "        ConvBlock-25             [-1, 35, 4, 4]               0\n",
            "           Linear-26                  [-1, 759]         425,799\n",
            "             ReLU-27                  [-1, 759]               0\n",
            "          Dropout-28                  [-1, 759]               0\n",
            "           Linear-29                   [-1, 28]          21,280\n",
            "================================================================\n",
            "Total params: 603,800\n",
            "Trainable params: 603,800\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 51.20\n",
            "Params size (MB): 2.30\n",
            "Estimated Total Size (MB): 53.56\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.334453  [   32/ 8582]\n",
            "Training loss: 3.275929  [ 1632/ 8582]\n",
            "Training loss: 3.065960  [ 3232/ 8582]\n",
            "Training loss: 2.929740  [ 4832/ 8582]\n",
            "Training loss: 2.948086  [ 6432/ 8582]\n",
            "Training loss: 2.921122  [ 8032/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:07:41,535] Trial 17 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 11.9%, Val - loss: 3.168973;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 18               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 74, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.175185678537983, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(74, 99, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5951406255254527, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(99, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.19344180680169826, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(34, 85, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.2861586078324915, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=5440, out_features=1634, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.34601683511117737, inplace=False)\n",
            "    (3): Linear(in_features=1634, out_features=1496, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.002537490388800575, inplace=False)\n",
            "    (6): Linear(in_features=1496, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 74, 128, 128]             740\n",
            "              ReLU-2         [-1, 74, 128, 128]               0\n",
            "           Dropout-3         [-1, 74, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 74, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 74, 64, 64]               0\n",
            "            Conv2d-6           [-1, 99, 64, 64]          66,033\n",
            "              ReLU-7           [-1, 99, 64, 64]               0\n",
            "           Dropout-8           [-1, 99, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 99, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 99, 32, 32]               0\n",
            "           Conv2d-11           [-1, 34, 32, 32]          30,328\n",
            "             ReLU-12           [-1, 34, 32, 32]               0\n",
            "          Dropout-13           [-1, 34, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 34, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 34, 16, 16]               0\n",
            "           Conv2d-16           [-1, 85, 16, 16]          26,095\n",
            "             ReLU-17           [-1, 85, 16, 16]               0\n",
            "          Dropout-18           [-1, 85, 16, 16]               0\n",
            "        MaxPool2d-19             [-1, 85, 8, 8]               0\n",
            "        ConvBlock-20             [-1, 85, 8, 8]               0\n",
            "           Linear-21                 [-1, 1634]       8,890,594\n",
            "             ReLU-22                 [-1, 1634]               0\n",
            "          Dropout-23                 [-1, 1634]               0\n",
            "           Linear-24                 [-1, 1496]       2,445,960\n",
            "             ReLU-25                 [-1, 1496]               0\n",
            "          Dropout-26                 [-1, 1496]               0\n",
            "           Linear-27                   [-1, 28]          41,916\n",
            "================================================================\n",
            "Total params: 11,501,666\n",
            "Trainable params: 11,501,666\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 44.79\n",
            "Params size (MB): 43.88\n",
            "Estimated Total Size (MB): 88.72\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.330539  [   32/ 8582]\n",
            "Training loss: 3.090234  [ 1632/ 8582]\n",
            "Training loss: 2.786083  [ 3232/ 8582]\n",
            "Training loss: 2.604580  [ 4832/ 8582]\n",
            "Training loss: 2.317842  [ 6432/ 8582]\n",
            "Training loss: 2.003562  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 39.2%, Val - loss: 2.230647;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=2 ------------------------------------------\n",
            "\n",
            "Training loss: 2.124308  [   32/ 8582]\n",
            "Training loss: 1.543890  [ 1632/ 8582]\n",
            "Training loss: 1.190145  [ 3232/ 8582]\n",
            "Training loss: 1.056722  [ 4832/ 8582]\n",
            "Training loss: 0.823804  [ 6432/ 8582]\n",
            "Training loss: 0.766246  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 80.9%, Val - loss: 1.133308;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=3 ------------------------------------------\n",
            "\n",
            "Training loss: 0.858584  [   32/ 8582]\n",
            "Training loss: 0.653086  [ 1632/ 8582]\n",
            "Training loss: 0.539995  [ 3232/ 8582]\n",
            "Training loss: 0.606654  [ 4832/ 8582]\n",
            "Training loss: 0.471538  [ 6432/ 8582]\n",
            "Training loss: 0.456959  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 87.8%, Val - loss: 0.922671;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=4 ------------------------------------------\n",
            "\n",
            "Training loss: 0.500705  [   32/ 8582]\n",
            "Training loss: 0.370947  [ 1632/ 8582]\n",
            "Training loss: 0.357080  [ 3232/ 8582]\n",
            "Training loss: 0.404933  [ 4832/ 8582]\n",
            "Training loss: 0.354912  [ 6432/ 8582]\n",
            "Training loss: 0.328514  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 89.0%, Val - loss: 0.767771;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=5 ------------------------------------------\n",
            "\n",
            "Training loss: 0.119718  [   32/ 8582]\n",
            "Training loss: 0.297550  [ 1632/ 8582]\n",
            "Training loss: 0.274066  [ 3232/ 8582]\n",
            "Training loss: 0.336813  [ 4832/ 8582]\n",
            "Training loss: 0.260745  [ 6432/ 8582]\n",
            "Training loss: 0.287469  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 89.0%, Val - loss: 0.806933;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=6 ------------------------------------------\n",
            "\n",
            "Training loss: 0.176861  [   32/ 8582]\n",
            "Training loss: 0.285484  [ 1632/ 8582]\n",
            "Training loss: 0.247727  [ 3232/ 8582]\n",
            "Training loss: 0.231308  [ 4832/ 8582]\n",
            "Training loss: 0.213116  [ 6432/ 8582]\n",
            "Training loss: 0.250077  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 89.6%, Val - loss: 0.680242;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=7 ------------------------------------------\n",
            "\n",
            "Training loss: 0.181007  [   32/ 8582]\n",
            "Training loss: 0.254050  [ 1632/ 8582]\n",
            "Training loss: 0.200756  [ 3232/ 8582]\n",
            "Training loss: 0.241980  [ 4832/ 8582]\n",
            "Training loss: 0.180016  [ 6432/ 8582]\n",
            "Training loss: 0.248233  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.9%, Val - loss: 0.512304;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=8 ------------------------------------------\n",
            "\n",
            "Training loss: 0.060022  [   32/ 8582]\n",
            "Training loss: 0.200434  [ 1632/ 8582]\n",
            "Training loss: 0.165448  [ 3232/ 8582]\n",
            "Training loss: 0.222706  [ 4832/ 8582]\n",
            "Training loss: 0.168510  [ 6432/ 8582]\n",
            "Training loss: 0.212551  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.5%, Val - loss: 0.502523;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=9 ------------------------------------------\n",
            "\n",
            "Training loss: 0.117441  [   32/ 8582]\n",
            "Training loss: 0.176144  [ 1632/ 8582]\n",
            "Training loss: 0.189284  [ 3232/ 8582]\n",
            "Training loss: 0.142350  [ 4832/ 8582]\n",
            "Training loss: 0.121589  [ 6432/ 8582]\n",
            "Training loss: 0.195500  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.2%, Val - loss: 0.593876;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=10 ------------------------------------------\n",
            "\n",
            "Training loss: 0.048661  [   32/ 8582]\n",
            "Training loss: 0.143000  [ 1632/ 8582]\n",
            "Training loss: 0.140413  [ 3232/ 8582]\n",
            "Training loss: 0.157943  [ 4832/ 8582]\n",
            "Training loss: 0.153496  [ 6432/ 8582]\n",
            "Training loss: 0.152952  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.7%, Val - loss: 0.477461;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=11 ------------------------------------------\n",
            "\n",
            "Training loss: 0.084546  [   32/ 8582]\n",
            "Training loss: 0.163501  [ 1632/ 8582]\n",
            "Training loss: 0.135399  [ 3232/ 8582]\n",
            "Training loss: 0.148093  [ 4832/ 8582]\n",
            "Training loss: 0.113628  [ 6432/ 8582]\n",
            "Training loss: 0.110895  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 89.6%, Val - loss: 0.576200;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=12 ------------------------------------------\n",
            "\n",
            "Training loss: 0.138508  [   32/ 8582]\n",
            "Training loss: 0.109315  [ 1632/ 8582]\n",
            "Training loss: 0.121516  [ 3232/ 8582]\n",
            "Training loss: 0.139688  [ 4832/ 8582]\n",
            "Training loss: 0.093843  [ 6432/ 8582]\n",
            "Training loss: 0.169479  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 88.3%, Val - loss: 0.590103;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=13 ------------------------------------------\n",
            "\n",
            "Training loss: 0.401168  [   32/ 8582]\n",
            "Training loss: 0.136746  [ 1632/ 8582]\n",
            "Training loss: 0.143625  [ 3232/ 8582]\n",
            "Training loss: 0.115801  [ 4832/ 8582]\n",
            "Training loss: 0.097608  [ 6432/ 8582]\n",
            "Training loss: 0.143977  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 89.7%, Val - loss: 0.447706;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=14 ------------------------------------------\n",
            "\n",
            "Training loss: 0.051377  [   32/ 8582]\n",
            "Training loss: 0.140032  [ 1632/ 8582]\n",
            "Training loss: 0.098928  [ 3232/ 8582]\n",
            "Training loss: 0.107453  [ 4832/ 8582]\n",
            "Training loss: 0.128430  [ 6432/ 8582]\n",
            "Training loss: 0.146765  [ 8032/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:14:10,341] Trial 18 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 91.0%, Val - loss: 0.450959;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 19               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 110, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.009304222789177474, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(110, 74, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.48986484796041563, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(74, 91, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.332458427081803, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=23296, out_features=321, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.6653612201352421, inplace=False)\n",
            "    (3): Linear(in_features=321, out_features=195, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.24896426574356403, inplace=False)\n",
            "    (6): Linear(in_features=195, out_features=1592, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.3053500966113473, inplace=False)\n",
            "    (9): Linear(in_features=1592, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 110, 128, 128]           1,100\n",
            "              ReLU-2        [-1, 110, 128, 128]               0\n",
            "           Dropout-3        [-1, 110, 128, 128]               0\n",
            "         MaxPool2d-4          [-1, 110, 64, 64]               0\n",
            "         ConvBlock-5          [-1, 110, 64, 64]               0\n",
            "            Conv2d-6           [-1, 74, 64, 64]          73,334\n",
            "              ReLU-7           [-1, 74, 64, 64]               0\n",
            "           Dropout-8           [-1, 74, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 74, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 74, 32, 32]               0\n",
            "           Conv2d-11           [-1, 91, 32, 32]          60,697\n",
            "             ReLU-12           [-1, 91, 32, 32]               0\n",
            "          Dropout-13           [-1, 91, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 91, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 91, 16, 16]               0\n",
            "           Linear-16                  [-1, 321]       7,478,337\n",
            "             ReLU-17                  [-1, 321]               0\n",
            "          Dropout-18                  [-1, 321]               0\n",
            "           Linear-19                  [-1, 195]          62,790\n",
            "             ReLU-20                  [-1, 195]               0\n",
            "          Dropout-21                  [-1, 195]               0\n",
            "           Linear-22                 [-1, 1592]         312,032\n",
            "             ReLU-23                 [-1, 1592]               0\n",
            "          Dropout-24                 [-1, 1592]               0\n",
            "           Linear-25                   [-1, 28]          44,604\n",
            "================================================================\n",
            "Total params: 8,032,894\n",
            "Trainable params: 8,032,894\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 58.76\n",
            "Params size (MB): 30.64\n",
            "Estimated Total Size (MB): 89.46\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.330079  [  128/ 8582]\n",
            "Training loss: 6579.872408  [ 6528/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:14:39,484] Trial 19 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 3.6%, Val - loss: 3.371912;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 20               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.3298767368219253, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(60, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.6227772340071466, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(59, 114, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.09757769795320975, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(114, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.09737952296500381, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (4): ConvBlock(\n",
            "      (conv): Conv2d(116, 74, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.3367940111598992, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=1184, out_features=658, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5005407439147704, inplace=False)\n",
            "    (3): Linear(in_features=658, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 60, 128, 128]             600\n",
            "              ReLU-2         [-1, 60, 128, 128]               0\n",
            "           Dropout-3         [-1, 60, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 60, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 60, 64, 64]               0\n",
            "            Conv2d-6           [-1, 59, 64, 64]          31,919\n",
            "              ReLU-7           [-1, 59, 64, 64]               0\n",
            "           Dropout-8           [-1, 59, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 59, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 59, 32, 32]               0\n",
            "           Conv2d-11          [-1, 114, 32, 32]          60,648\n",
            "             ReLU-12          [-1, 114, 32, 32]               0\n",
            "          Dropout-13          [-1, 114, 32, 32]               0\n",
            "        MaxPool2d-14          [-1, 114, 16, 16]               0\n",
            "        ConvBlock-15          [-1, 114, 16, 16]               0\n",
            "           Conv2d-16          [-1, 116, 16, 16]         119,132\n",
            "             ReLU-17          [-1, 116, 16, 16]               0\n",
            "          Dropout-18          [-1, 116, 16, 16]               0\n",
            "        MaxPool2d-19            [-1, 116, 8, 8]               0\n",
            "        ConvBlock-20            [-1, 116, 8, 8]               0\n",
            "           Conv2d-21             [-1, 74, 8, 8]          77,330\n",
            "             ReLU-22             [-1, 74, 8, 8]               0\n",
            "          Dropout-23             [-1, 74, 8, 8]               0\n",
            "        MaxPool2d-24             [-1, 74, 4, 4]               0\n",
            "        ConvBlock-25             [-1, 74, 4, 4]               0\n",
            "           Linear-26                  [-1, 658]         779,730\n",
            "             ReLU-27                  [-1, 658]               0\n",
            "          Dropout-28                  [-1, 658]               0\n",
            "           Linear-29                   [-1, 28]          18,452\n",
            "================================================================\n",
            "Total params: 1,087,811\n",
            "Trainable params: 1,087,811\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 36.76\n",
            "Params size (MB): 4.15\n",
            "Estimated Total Size (MB): 40.97\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.330995  [   64/ 8582]\n",
            "Training loss: 3.330849  [ 3264/ 8582]\n",
            "Training loss: 3.334266  [ 6464/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:15:08,363] Trial 20 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 3.4%, Val - loss: 3.333572;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 21               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 98, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.17666536436879762, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(98, 91, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5650844691328641, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(91, 74, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.4597505970483252, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(74, 98, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.027219015930178628, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=6272, out_features=490, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2870103501826177, inplace=False)\n",
            "    (3): Linear(in_features=490, out_features=1026, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.0999929991478419, inplace=False)\n",
            "    (6): Linear(in_features=1026, out_features=156, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.38312768303688555, inplace=False)\n",
            "    (9): Linear(in_features=156, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 98, 128, 128]             980\n",
            "              ReLU-2         [-1, 98, 128, 128]               0\n",
            "           Dropout-3         [-1, 98, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 98, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 98, 64, 64]               0\n",
            "            Conv2d-6           [-1, 91, 64, 64]          80,353\n",
            "              ReLU-7           [-1, 91, 64, 64]               0\n",
            "           Dropout-8           [-1, 91, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 91, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 91, 32, 32]               0\n",
            "           Conv2d-11           [-1, 74, 32, 32]          60,680\n",
            "             ReLU-12           [-1, 74, 32, 32]               0\n",
            "          Dropout-13           [-1, 74, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 74, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 74, 16, 16]               0\n",
            "           Conv2d-16           [-1, 98, 16, 16]          65,366\n",
            "             ReLU-17           [-1, 98, 16, 16]               0\n",
            "          Dropout-18           [-1, 98, 16, 16]               0\n",
            "        MaxPool2d-19             [-1, 98, 8, 8]               0\n",
            "        ConvBlock-20             [-1, 98, 8, 8]               0\n",
            "           Linear-21                  [-1, 490]       3,073,770\n",
            "             ReLU-22                  [-1, 490]               0\n",
            "          Dropout-23                  [-1, 490]               0\n",
            "           Linear-24                 [-1, 1026]         503,766\n",
            "             ReLU-25                 [-1, 1026]               0\n",
            "          Dropout-26                 [-1, 1026]               0\n",
            "           Linear-27                  [-1, 156]         160,212\n",
            "             ReLU-28                  [-1, 156]               0\n",
            "          Dropout-29                  [-1, 156]               0\n",
            "           Linear-30                   [-1, 28]           4,396\n",
            "================================================================\n",
            "Total params: 3,949,523\n",
            "Trainable params: 3,949,523\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 55.56\n",
            "Params size (MB): 15.07\n",
            "Estimated Total Size (MB): 70.69\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.328510  [  128/ 8582]\n",
            "Training loss: 3.014178  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 19.7%, Val - loss: 3.045131;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=2 ------------------------------------------\n",
            "\n",
            "Training loss: 2.724566  [  128/ 8582]\n",
            "Training loss: 2.636818  [ 6528/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:16:04,497] Trial 21 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 31.6%, Val - loss: 2.736374;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 22               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 110, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.22346913636509164, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(110, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5371508542567045, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(100, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.4226952361926556, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(76, 95, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.022142306925029533, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=6080, out_features=378, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.22926237492933016, inplace=False)\n",
            "    (3): Linear(in_features=378, out_features=1684, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.03213158076006867, inplace=False)\n",
            "    (6): Linear(in_features=1684, out_features=1481, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.40589014053642486, inplace=False)\n",
            "    (9): Linear(in_features=1481, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 110, 128, 128]           1,100\n",
            "              ReLU-2        [-1, 110, 128, 128]               0\n",
            "           Dropout-3        [-1, 110, 128, 128]               0\n",
            "         MaxPool2d-4          [-1, 110, 64, 64]               0\n",
            "         ConvBlock-5          [-1, 110, 64, 64]               0\n",
            "            Conv2d-6          [-1, 100, 64, 64]          99,100\n",
            "              ReLU-7          [-1, 100, 64, 64]               0\n",
            "           Dropout-8          [-1, 100, 64, 64]               0\n",
            "         MaxPool2d-9          [-1, 100, 32, 32]               0\n",
            "        ConvBlock-10          [-1, 100, 32, 32]               0\n",
            "           Conv2d-11           [-1, 76, 32, 32]          68,476\n",
            "             ReLU-12           [-1, 76, 32, 32]               0\n",
            "          Dropout-13           [-1, 76, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 76, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 76, 16, 16]               0\n",
            "           Conv2d-16           [-1, 95, 16, 16]          65,075\n",
            "             ReLU-17           [-1, 95, 16, 16]               0\n",
            "          Dropout-18           [-1, 95, 16, 16]               0\n",
            "        MaxPool2d-19             [-1, 95, 8, 8]               0\n",
            "        ConvBlock-20             [-1, 95, 8, 8]               0\n",
            "           Linear-21                  [-1, 378]       2,298,618\n",
            "             ReLU-22                  [-1, 378]               0\n",
            "          Dropout-23                  [-1, 378]               0\n",
            "           Linear-24                 [-1, 1684]         638,236\n",
            "             ReLU-25                 [-1, 1684]               0\n",
            "          Dropout-26                 [-1, 1684]               0\n",
            "           Linear-27                 [-1, 1481]       2,495,485\n",
            "             ReLU-28                 [-1, 1481]               0\n",
            "          Dropout-29                 [-1, 1481]               0\n",
            "           Linear-30                   [-1, 28]          41,496\n",
            "================================================================\n",
            "Total params: 5,707,586\n",
            "Trainable params: 5,707,586\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 61.87\n",
            "Params size (MB): 21.77\n",
            "Estimated Total Size (MB): 83.71\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.332088  [  128/ 8582]\n",
            "Training loss: 2.986382  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 18.5%, Val - loss: 2.887052;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=2 ------------------------------------------\n",
            "\n",
            "Training loss: 2.697877  [  128/ 8582]\n",
            "Training loss: 2.495517  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 36.1%, Val - loss: 2.516150;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=3 ------------------------------------------\n",
            "\n",
            "Training loss: 2.336488  [  128/ 8582]\n",
            "Training loss: 1.665975  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 69.2%, Val - loss: 1.848885;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=4 ------------------------------------------\n",
            "\n",
            "Training loss: 1.223514  [  128/ 8582]\n",
            "Training loss: 0.941513  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 80.6%, Val - loss: 1.384793;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=5 ------------------------------------------\n",
            "\n",
            "Training loss: 0.741658  [  128/ 8582]\n",
            "Training loss: 0.620537  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 84.0%, Val - loss: 1.237277;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=6 ------------------------------------------\n",
            "\n",
            "Training loss: 0.554273  [  128/ 8582]\n",
            "Training loss: 0.518505  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 86.6%, Val - loss: 1.012524;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=7 ------------------------------------------\n",
            "\n",
            "Training loss: 0.390556  [  128/ 8582]\n",
            "Training loss: 0.380306  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 87.5%, Val - loss: 0.878380;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=8 ------------------------------------------\n",
            "\n",
            "Training loss: 0.338542  [  128/ 8582]\n",
            "Training loss: 0.329216  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 87.7%, Val - loss: 0.874865;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=9 ------------------------------------------\n",
            "\n",
            "Training loss: 0.321471  [  128/ 8582]\n",
            "Training loss: 0.273975  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 89.5%, Val - loss: 0.745256;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=10 ------------------------------------------\n",
            "\n",
            "Training loss: 0.238739  [  128/ 8582]\n",
            "Training loss: 0.211386  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.3%, Val - loss: 0.618275;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=11 ------------------------------------------\n",
            "\n",
            "Training loss: 0.187744  [  128/ 8582]\n",
            "Training loss: 0.180397  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.3%, Val - loss: 0.551855;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=12 ------------------------------------------\n",
            "\n",
            "Training loss: 0.167873  [  128/ 8582]\n",
            "Training loss: 0.171543  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.1%, Val - loss: 0.534144;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=13 ------------------------------------------\n",
            "\n",
            "Training loss: 0.209731  [  128/ 8582]\n",
            "Training loss: 0.145127  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.7%, Val - loss: 0.552436;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=14 ------------------------------------------\n",
            "\n",
            "Training loss: 0.173105  [  128/ 8582]\n",
            "Training loss: 0.144246  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.9%, Val - loss: 0.508888;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=15 ------------------------------------------\n",
            "\n",
            "Training loss: 0.115093  [  128/ 8582]\n",
            "Training loss: 0.112250  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.0%, Val - loss: 0.563269;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=16 ------------------------------------------\n",
            "\n",
            "Training loss: 0.188951  [  128/ 8582]\n",
            "Training loss: 0.114346  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.1%, Val - loss: 0.482782;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=17 ------------------------------------------\n",
            "\n",
            "Training loss: 0.114430  [  128/ 8582]\n",
            "Training loss: 0.089643  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.8%, Val - loss: 0.481361;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=18 ------------------------------------------\n",
            "\n",
            "Training loss: 0.072344  [  128/ 8582]\n",
            "Training loss: 0.112110  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.3%, Val - loss: 0.406360;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=19 ------------------------------------------\n",
            "\n",
            "Training loss: 0.032787  [  128/ 8582]\n",
            "Training loss: 0.094985  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 89.5%, Val - loss: 0.623814;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=20 ------------------------------------------\n",
            "\n",
            "Training loss: 0.249370  [  128/ 8582]\n",
            "Training loss: 0.100901  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.2%, Val - loss: 0.505804;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=21 ------------------------------------------\n",
            "\n",
            "Training loss: 0.139832  [  128/ 8582]\n",
            "Training loss: 0.089764  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.1%, Val - loss: 0.436963;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=22 ------------------------------------------\n",
            "\n",
            "Training loss: 0.125476  [  128/ 8582]\n",
            "Training loss: 0.074516  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.1%, Val - loss: 0.476090;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=23 ------------------------------------------\n",
            "\n",
            "Training loss: 0.107232  [  128/ 8582]\n",
            "Training loss: 0.082872  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.2%, Val - loss: 0.392436;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=24 ------------------------------------------\n",
            "\n",
            "Training loss: 0.122963  [  128/ 8582]\n",
            "Training loss: 0.076796  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.5%, Val - loss: 0.414702;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=25 ------------------------------------------\n",
            "\n",
            "Training loss: 0.034978  [  128/ 8582]\n",
            "Training loss: 0.065020  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.6%, Val - loss: 0.406101;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=26 ------------------------------------------\n",
            "\n",
            "Training loss: 0.098222  [  128/ 8582]\n",
            "Training loss: 0.088539  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.1%, Val - loss: 0.416770;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=27 ------------------------------------------\n",
            "\n",
            "Training loss: 0.086430  [  128/ 8582]\n",
            "Training loss: 0.091729  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.5%, Val - loss: 0.391594;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=28 ------------------------------------------\n",
            "\n",
            "Training loss: 0.129153  [  128/ 8582]\n",
            "Training loss: 0.079183  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.4%, Val - loss: 0.402807;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=29 ------------------------------------------\n",
            "\n",
            "Training loss: 0.045412  [  128/ 8582]\n",
            "Training loss: 0.053607  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.1%, Val - loss: 0.356651;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=30 ------------------------------------------\n",
            "\n",
            "Training loss: 0.029888  [  128/ 8582]\n",
            "Training loss: 0.061040  [ 6528/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:29:46,518] Trial 22 finished with value: 0.9287656334964655 and parameters: {'nr_conv_blocks': 4, 'out_channels_0': 110, 'out_channels_1': 100, 'out_channels_2': 76, 'out_channels_3': 95, 'dropout_rate_0': 0.22346913636509164, 'dropout_rate_1': 0.5371508542567045, 'dropout_rate_2': 0.4226952361926556, 'dropout_rate_3': 0.022142306925029533, 'nr_fc_layers': 3, 'fc_size_0': 378, 'fc_size_1': 1684, 'fc_size_2': 1481, 'dropout_rate_fc_0': 0.22926237492933016, 'dropout_rate_fc_1': 0.03213158076006867, 'dropout_rate_fc_2': 0.40589014053642486, 'optimizer': 'Adam', 'lr': 0.0005653206498768188, 'batch_size': 128}. Best is trial 3 with value: 0.9467101685698749.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 92.9%, Val - loss: 0.406105;\n",
            "\n",
            "#############################################\n",
            "#               Finished trial - 22/30               #\n",
            "#############################################\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 23               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.14550094979873462, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(92, 78, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.6428066558407963, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(78, 81, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.2794614994881668, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(81, 105, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.010819017015648935, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=6720, out_features=911, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.38187277828568916, inplace=False)\n",
            "    (3): Linear(in_features=911, out_features=1312, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.1310069822492157, inplace=False)\n",
            "    (6): Linear(in_features=1312, out_features=711, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.5097658333555832, inplace=False)\n",
            "    (9): Linear(in_features=711, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 92, 128, 128]             920\n",
            "              ReLU-2         [-1, 92, 128, 128]               0\n",
            "           Dropout-3         [-1, 92, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 92, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 92, 64, 64]               0\n",
            "            Conv2d-6           [-1, 78, 64, 64]          64,662\n",
            "              ReLU-7           [-1, 78, 64, 64]               0\n",
            "           Dropout-8           [-1, 78, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 78, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 78, 32, 32]               0\n",
            "           Conv2d-11           [-1, 81, 32, 32]          56,943\n",
            "             ReLU-12           [-1, 81, 32, 32]               0\n",
            "          Dropout-13           [-1, 81, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 81, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 81, 16, 16]               0\n",
            "           Conv2d-16          [-1, 105, 16, 16]          76,650\n",
            "             ReLU-17          [-1, 105, 16, 16]               0\n",
            "          Dropout-18          [-1, 105, 16, 16]               0\n",
            "        MaxPool2d-19            [-1, 105, 8, 8]               0\n",
            "        ConvBlock-20            [-1, 105, 8, 8]               0\n",
            "           Linear-21                  [-1, 911]       6,122,831\n",
            "             ReLU-22                  [-1, 911]               0\n",
            "          Dropout-23                  [-1, 911]               0\n",
            "           Linear-24                 [-1, 1312]       1,196,544\n",
            "             ReLU-25                 [-1, 1312]               0\n",
            "          Dropout-26                 [-1, 1312]               0\n",
            "           Linear-27                  [-1, 711]         933,543\n",
            "             ReLU-28                  [-1, 711]               0\n",
            "          Dropout-29                  [-1, 711]               0\n",
            "           Linear-30                   [-1, 28]          19,936\n",
            "================================================================\n",
            "Total params: 8,472,029\n",
            "Trainable params: 8,472,029\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 51.78\n",
            "Params size (MB): 32.32\n",
            "Estimated Total Size (MB): 84.16\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.328337  [  128/ 8582]\n",
            "Training loss: 3.031667  [ 6528/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:30:15,180] Trial 23 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 16.5%, Val - loss: 3.052850;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 24               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 79, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.10049513641015867, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(79, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.68853479583213, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(127, 67, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.537817807017892, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(67, 81, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.1110949798445641, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=5184, out_features=1217, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2996349033754115, inplace=False)\n",
            "    (3): Linear(in_features=1217, out_features=854, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.003658600549988891, inplace=False)\n",
            "    (6): Linear(in_features=854, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 79, 128, 128]             790\n",
            "              ReLU-2         [-1, 79, 128, 128]               0\n",
            "           Dropout-3         [-1, 79, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 79, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 79, 64, 64]               0\n",
            "            Conv2d-6          [-1, 127, 64, 64]          90,424\n",
            "              ReLU-7          [-1, 127, 64, 64]               0\n",
            "           Dropout-8          [-1, 127, 64, 64]               0\n",
            "         MaxPool2d-9          [-1, 127, 32, 32]               0\n",
            "        ConvBlock-10          [-1, 127, 32, 32]               0\n",
            "           Conv2d-11           [-1, 67, 32, 32]          76,648\n",
            "             ReLU-12           [-1, 67, 32, 32]               0\n",
            "          Dropout-13           [-1, 67, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 67, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 67, 16, 16]               0\n",
            "           Conv2d-16           [-1, 81, 16, 16]          48,924\n",
            "             ReLU-17           [-1, 81, 16, 16]               0\n",
            "          Dropout-18           [-1, 81, 16, 16]               0\n",
            "        MaxPool2d-19             [-1, 81, 8, 8]               0\n",
            "        ConvBlock-20             [-1, 81, 8, 8]               0\n",
            "           Linear-21                 [-1, 1217]       6,310,145\n",
            "             ReLU-22                 [-1, 1217]               0\n",
            "          Dropout-23                 [-1, 1217]               0\n",
            "           Linear-24                  [-1, 854]       1,040,172\n",
            "             ReLU-25                  [-1, 854]               0\n",
            "          Dropout-26                  [-1, 854]               0\n",
            "           Linear-27                   [-1, 28]          23,940\n",
            "================================================================\n",
            "Total params: 7,591,043\n",
            "Trainable params: 7,591,043\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 50.89\n",
            "Params size (MB): 28.96\n",
            "Estimated Total Size (MB): 79.91\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.335627  [  128/ 8582]\n",
            "Training loss: 2.952040  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 22.1%, Val - loss: 2.962703;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=2 ------------------------------------------\n",
            "\n",
            "Training loss: 2.506995  [  128/ 8582]\n",
            "Training loss: 1.898413  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 66.6%, Val - loss: 2.273831;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=3 ------------------------------------------\n",
            "\n",
            "Training loss: 1.214187  [  128/ 8582]\n",
            "Training loss: 0.902995  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 79.4%, Val - loss: 1.648099;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=4 ------------------------------------------\n",
            "\n",
            "Training loss: 0.584097  [  128/ 8582]\n",
            "Training loss: 0.522038  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 83.8%, Val - loss: 1.338801;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=5 ------------------------------------------\n",
            "\n",
            "Training loss: 0.370436  [  128/ 8582]\n",
            "Training loss: 0.384273  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 85.6%, Val - loss: 1.145426;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=6 ------------------------------------------\n",
            "\n",
            "Training loss: 0.231294  [  128/ 8582]\n",
            "Training loss: 0.275019  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 88.3%, Val - loss: 0.978321;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=7 ------------------------------------------\n",
            "\n",
            "Training loss: 0.283037  [  128/ 8582]\n",
            "Training loss: 0.231223  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 87.8%, Val - loss: 0.846187;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=8 ------------------------------------------\n",
            "\n",
            "Training loss: 0.183474  [  128/ 8582]\n",
            "Training loss: 0.224455  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 88.4%, Val - loss: 0.761523;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=9 ------------------------------------------\n",
            "\n",
            "Training loss: 0.172677  [  128/ 8582]\n",
            "Training loss: 0.189418  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.4%, Val - loss: 0.811636;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=10 ------------------------------------------\n",
            "\n",
            "Training loss: 0.184615  [  128/ 8582]\n",
            "Training loss: 0.155344  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 88.9%, Val - loss: 0.866084;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=11 ------------------------------------------\n",
            "\n",
            "Training loss: 0.112615  [  128/ 8582]\n",
            "Training loss: 0.143227  [ 6528/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:35:15,424] Trial 24 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 87.1%, Val - loss: 0.821531;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 25               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 101, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.22671143900235458, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(101, 95, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5587086745243445, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(95, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.22921813054333795, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=24576, out_features=538, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.21977743274527745, inplace=False)\n",
            "    (3): Linear(in_features=538, out_features=1260, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.15772391388713344, inplace=False)\n",
            "    (6): Linear(in_features=1260, out_features=1516, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.30881145847801167, inplace=False)\n",
            "    (9): Linear(in_features=1516, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 101, 128, 128]           1,010\n",
            "              ReLU-2        [-1, 101, 128, 128]               0\n",
            "           Dropout-3        [-1, 101, 128, 128]               0\n",
            "         MaxPool2d-4          [-1, 101, 64, 64]               0\n",
            "         ConvBlock-5          [-1, 101, 64, 64]               0\n",
            "            Conv2d-6           [-1, 95, 64, 64]          86,450\n",
            "              ReLU-7           [-1, 95, 64, 64]               0\n",
            "           Dropout-8           [-1, 95, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 95, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 95, 32, 32]               0\n",
            "           Conv2d-11           [-1, 96, 32, 32]          82,176\n",
            "             ReLU-12           [-1, 96, 32, 32]               0\n",
            "          Dropout-13           [-1, 96, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 96, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 96, 16, 16]               0\n",
            "           Linear-16                  [-1, 538]      13,222,426\n",
            "             ReLU-17                  [-1, 538]               0\n",
            "          Dropout-18                  [-1, 538]               0\n",
            "           Linear-19                 [-1, 1260]         679,140\n",
            "             ReLU-20                 [-1, 1260]               0\n",
            "          Dropout-21                 [-1, 1260]               0\n",
            "           Linear-22                 [-1, 1516]       1,911,676\n",
            "             ReLU-23                 [-1, 1516]               0\n",
            "          Dropout-24                 [-1, 1516]               0\n",
            "           Linear-25                   [-1, 28]          42,476\n",
            "================================================================\n",
            "Total params: 16,025,354\n",
            "Trainable params: 16,025,354\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 57.28\n",
            "Params size (MB): 61.13\n",
            "Estimated Total Size (MB): 118.47\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.337011  [   32/ 8582]\n",
            "Training loss: 3.118209  [ 1632/ 8582]\n",
            "Training loss: 2.937531  [ 3232/ 8582]\n",
            "Training loss: 2.777050  [ 4832/ 8582]\n",
            "Training loss: 2.416313  [ 6432/ 8582]\n",
            "Training loss: 2.055596  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 55.4%, Val - loss: 2.149031;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=2 ------------------------------------------\n",
            "\n",
            "Training loss: 2.022236  [   32/ 8582]\n",
            "Training loss: 1.477361  [ 1632/ 8582]\n",
            "Training loss: 1.137997  [ 3232/ 8582]\n",
            "Training loss: 0.879298  [ 4832/ 8582]\n",
            "Training loss: 0.702015  [ 6432/ 8582]\n",
            "Training loss: 0.635978  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 86.9%, Val - loss: 1.059798;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=3 ------------------------------------------\n",
            "\n",
            "Training loss: 1.064685  [   32/ 8582]\n",
            "Training loss: 0.551731  [ 1632/ 8582]\n",
            "Training loss: 0.470768  [ 3232/ 8582]\n",
            "Training loss: 0.407802  [ 4832/ 8582]\n",
            "Training loss: 0.353857  [ 6432/ 8582]\n",
            "Training loss: 0.379938  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 90.3%, Val - loss: 0.833043;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=4 ------------------------------------------\n",
            "\n",
            "Training loss: 0.471063  [   32/ 8582]\n",
            "Training loss: 0.324190  [ 1632/ 8582]\n",
            "Training loss: 0.331467  [ 3232/ 8582]\n",
            "Training loss: 0.247349  [ 4832/ 8582]\n",
            "Training loss: 0.264293  [ 6432/ 8582]\n",
            "Training loss: 0.255982  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.4%, Val - loss: 0.597194;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=5 ------------------------------------------\n",
            "\n",
            "Training loss: 0.325546  [   32/ 8582]\n",
            "Training loss: 0.209384  [ 1632/ 8582]\n",
            "Training loss: 0.243636  [ 3232/ 8582]\n",
            "Training loss: 0.187862  [ 4832/ 8582]\n",
            "Training loss: 0.187307  [ 6432/ 8582]\n",
            "Training loss: 0.197560  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 91.6%, Val - loss: 0.570794;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=6 ------------------------------------------\n",
            "\n",
            "Training loss: 0.531861  [   32/ 8582]\n",
            "Training loss: 0.179272  [ 1632/ 8582]\n",
            "Training loss: 0.185333  [ 3232/ 8582]\n",
            "Training loss: 0.158455  [ 4832/ 8582]\n",
            "Training loss: 0.155896  [ 6432/ 8582]\n",
            "Training loss: 0.165349  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.8%, Val - loss: 0.492123;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=7 ------------------------------------------\n",
            "\n",
            "Training loss: 0.165808  [   32/ 8582]\n",
            "Training loss: 0.101830  [ 1632/ 8582]\n",
            "Training loss: 0.146080  [ 3232/ 8582]\n",
            "Training loss: 0.132363  [ 4832/ 8582]\n",
            "Training loss: 0.154420  [ 6432/ 8582]\n",
            "Training loss: 0.147405  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.4%, Val - loss: 0.471840;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=8 ------------------------------------------\n",
            "\n",
            "Training loss: 0.123790  [   32/ 8582]\n",
            "Training loss: 0.154602  [ 1632/ 8582]\n",
            "Training loss: 0.115626  [ 3232/ 8582]\n",
            "Training loss: 0.098919  [ 4832/ 8582]\n",
            "Training loss: 0.123806  [ 6432/ 8582]\n",
            "Training loss: 0.147081  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.8%, Val - loss: 0.416320;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=9 ------------------------------------------\n",
            "\n",
            "Training loss: 0.211913  [   32/ 8582]\n",
            "Training loss: 0.077900  [ 1632/ 8582]\n",
            "Training loss: 0.086886  [ 3232/ 8582]\n",
            "Training loss: 0.101134  [ 4832/ 8582]\n",
            "Training loss: 0.106769  [ 6432/ 8582]\n",
            "Training loss: 0.106706  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.8%, Val - loss: 0.414695;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=10 ------------------------------------------\n",
            "\n",
            "Training loss: 0.116327  [   32/ 8582]\n",
            "Training loss: 0.083067  [ 1632/ 8582]\n",
            "Training loss: 0.078336  [ 3232/ 8582]\n",
            "Training loss: 0.065319  [ 4832/ 8582]\n",
            "Training loss: 0.110635  [ 6432/ 8582]\n",
            "Training loss: 0.110372  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.5%, Val - loss: 0.413219;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=11 ------------------------------------------\n",
            "\n",
            "Training loss: 0.109251  [   32/ 8582]\n",
            "Training loss: 0.057590  [ 1632/ 8582]\n",
            "Training loss: 0.097433  [ 3232/ 8582]\n",
            "Training loss: 0.070465  [ 4832/ 8582]\n",
            "Training loss: 0.076541  [ 6432/ 8582]\n",
            "Training loss: 0.085348  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.1%, Val - loss: 0.315788;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=12 ------------------------------------------\n",
            "\n",
            "Training loss: 0.172283  [   32/ 8582]\n",
            "Training loss: 0.089445  [ 1632/ 8582]\n",
            "Training loss: 0.072647  [ 3232/ 8582]\n",
            "Training loss: 0.076749  [ 4832/ 8582]\n",
            "Training loss: 0.102643  [ 6432/ 8582]\n",
            "Training loss: 0.073134  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.6%, Val - loss: 0.369910;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=13 ------------------------------------------\n",
            "\n",
            "Training loss: 0.191036  [   32/ 8582]\n",
            "Training loss: 0.086738  [ 1632/ 8582]\n",
            "Training loss: 0.072405  [ 3232/ 8582]\n",
            "Training loss: 0.056161  [ 4832/ 8582]\n",
            "Training loss: 0.061958  [ 6432/ 8582]\n",
            "Training loss: 0.066388  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.0%, Val - loss: 0.360866;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=14 ------------------------------------------\n",
            "\n",
            "Training loss: 0.220913  [   32/ 8582]\n",
            "Training loss: 0.052159  [ 1632/ 8582]\n",
            "Training loss: 0.049600  [ 3232/ 8582]\n",
            "Training loss: 0.050830  [ 4832/ 8582]\n",
            "Training loss: 0.072879  [ 6432/ 8582]\n",
            "Training loss: 0.071527  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.5%, Val - loss: 0.297248;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=15 ------------------------------------------\n",
            "\n",
            "Training loss: 0.037567  [   32/ 8582]\n",
            "Training loss: 0.056712  [ 1632/ 8582]\n",
            "Training loss: 0.046772  [ 3232/ 8582]\n",
            "Training loss: 0.035985  [ 4832/ 8582]\n",
            "Training loss: 0.063290  [ 6432/ 8582]\n",
            "Training loss: 0.065089  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.3%, Val - loss: 0.311858;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=16 ------------------------------------------\n",
            "\n",
            "Training loss: 0.086300  [   32/ 8582]\n",
            "Training loss: 0.040784  [ 1632/ 8582]\n",
            "Training loss: 0.055059  [ 3232/ 8582]\n",
            "Training loss: 0.042835  [ 4832/ 8582]\n",
            "Training loss: 0.031003  [ 6432/ 8582]\n",
            "Training loss: 0.055430  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.6%, Val - loss: 0.277101;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=17 ------------------------------------------\n",
            "\n",
            "Training loss: 0.119374  [   32/ 8582]\n",
            "Training loss: 0.040407  [ 1632/ 8582]\n",
            "Training loss: 0.051277  [ 3232/ 8582]\n",
            "Training loss: 0.068683  [ 4832/ 8582]\n",
            "Training loss: 0.070257  [ 6432/ 8582]\n",
            "Training loss: 0.062819  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.3%, Val - loss: 0.281491;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=18 ------------------------------------------\n",
            "\n",
            "Training loss: 0.022166  [   32/ 8582]\n",
            "Training loss: 0.051914  [ 1632/ 8582]\n",
            "Training loss: 0.036598  [ 3232/ 8582]\n",
            "Training loss: 0.056181  [ 4832/ 8582]\n",
            "Training loss: 0.045799  [ 6432/ 8582]\n",
            "Training loss: 0.041562  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.4%, Val - loss: 0.280721;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=19 ------------------------------------------\n",
            "\n",
            "Training loss: 0.161982  [   32/ 8582]\n",
            "Training loss: 0.024045  [ 1632/ 8582]\n",
            "Training loss: 0.031172  [ 3232/ 8582]\n",
            "Training loss: 0.035451  [ 4832/ 8582]\n",
            "Training loss: 0.042340  [ 6432/ 8582]\n",
            "Training loss: 0.036770  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.1%, Val - loss: 0.320031;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=20 ------------------------------------------\n",
            "\n",
            "Training loss: 0.136011  [   32/ 8582]\n",
            "Training loss: 0.054453  [ 1632/ 8582]\n",
            "Training loss: 0.030825  [ 3232/ 8582]\n",
            "Training loss: 0.039069  [ 4832/ 8582]\n",
            "Training loss: 0.033434  [ 6432/ 8582]\n",
            "Training loss: 0.051377  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.7%, Val - loss: 0.294959;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=21 ------------------------------------------\n",
            "\n",
            "Training loss: 0.044056  [   32/ 8582]\n",
            "Training loss: 0.043085  [ 1632/ 8582]\n",
            "Training loss: 0.064271  [ 3232/ 8582]\n",
            "Training loss: 0.029801  [ 4832/ 8582]\n",
            "Training loss: 0.035976  [ 6432/ 8582]\n",
            "Training loss: 0.075542  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.2%, Val - loss: 0.276870;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=22 ------------------------------------------\n",
            "\n",
            "Training loss: 0.041519  [   32/ 8582]\n",
            "Training loss: 0.037326  [ 1632/ 8582]\n",
            "Training loss: 0.037919  [ 3232/ 8582]\n",
            "Training loss: 0.038471  [ 4832/ 8582]\n",
            "Training loss: 0.064077  [ 6432/ 8582]\n",
            "Training loss: 0.033302  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.9%, Val - loss: 0.257992;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=23 ------------------------------------------\n",
            "\n",
            "Training loss: 0.013734  [   32/ 8582]\n",
            "Training loss: 0.040741  [ 1632/ 8582]\n",
            "Training loss: 0.030785  [ 3232/ 8582]\n",
            "Training loss: 0.048238  [ 4832/ 8582]\n",
            "Training loss: 0.048288  [ 6432/ 8582]\n",
            "Training loss: 0.026664  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.5%, Val - loss: 0.229757;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=24 ------------------------------------------\n",
            "\n",
            "Training loss: 0.009387  [   32/ 8582]\n",
            "Training loss: 0.034678  [ 1632/ 8582]\n",
            "Training loss: 0.046936  [ 3232/ 8582]\n",
            "Training loss: 0.027871  [ 4832/ 8582]\n",
            "Training loss: 0.026826  [ 6432/ 8582]\n",
            "Training loss: 0.030349  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.6%, Val - loss: 0.225360;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=25 ------------------------------------------\n",
            "\n",
            "Training loss: 0.017542  [   32/ 8582]\n",
            "Training loss: 0.021763  [ 1632/ 8582]\n",
            "Training loss: 0.029648  [ 3232/ 8582]\n",
            "Training loss: 0.044703  [ 4832/ 8582]\n",
            "Training loss: 0.032320  [ 6432/ 8582]\n",
            "Training loss: 0.021741  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 93.6%, Val - loss: 0.241276;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=26 ------------------------------------------\n",
            "\n",
            "Training loss: 0.009045  [   32/ 8582]\n",
            "Training loss: 0.023749  [ 1632/ 8582]\n",
            "Training loss: 0.036342  [ 3232/ 8582]\n",
            "Training loss: 0.026522  [ 4832/ 8582]\n",
            "Training loss: 0.025736  [ 6432/ 8582]\n",
            "Training loss: 0.026202  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.6%, Val - loss: 0.253062;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=27 ------------------------------------------\n",
            "\n",
            "Training loss: 0.023124  [   32/ 8582]\n",
            "Training loss: 0.044388  [ 1632/ 8582]\n",
            "Training loss: 0.042372  [ 3232/ 8582]\n",
            "Training loss: 0.048879  [ 4832/ 8582]\n",
            "Training loss: 0.032428  [ 6432/ 8582]\n",
            "Training loss: 0.030161  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.9%, Val - loss: 0.236475;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=28 ------------------------------------------\n",
            "\n",
            "Training loss: 0.029359  [   32/ 8582]\n",
            "Training loss: 0.017803  [ 1632/ 8582]\n",
            "Training loss: 0.026872  [ 3232/ 8582]\n",
            "Training loss: 0.023505  [ 4832/ 8582]\n",
            "Training loss: 0.016891  [ 6432/ 8582]\n",
            "Training loss: 0.012062  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.2%, Val - loss: 0.215990;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=29 ------------------------------------------\n",
            "\n",
            "Training loss: 0.004005  [   32/ 8582]\n",
            "Training loss: 0.029476  [ 1632/ 8582]\n",
            "Training loss: 0.015073  [ 3232/ 8582]\n",
            "Training loss: 0.013204  [ 4832/ 8582]\n",
            "Training loss: 0.021157  [ 6432/ 8582]\n",
            "Training loss: 0.035609  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 94.0%, Val - loss: 0.239468;\n",
            "\n",
            "\n",
            "------------------------------------------ Epoch=30 ------------------------------------------\n",
            "\n",
            "Training loss: 0.022634  [   32/ 8582]\n",
            "Training loss: 0.023317  [ 1632/ 8582]\n",
            "Training loss: 0.026104  [ 3232/ 8582]\n",
            "Training loss: 0.028307  [ 4832/ 8582]\n",
            "Training loss: 0.058633  [ 6432/ 8582]\n",
            "Training loss: 0.033513  [ 8032/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 92.7%, Val - loss: 0.279139;\n",
            "\n",
            "#############################################\n",
            "#               Finished trial - 25/30               #\n",
            "#############################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:49:10,548] Trial 25 finished with value: 0.9265905383360522 and parameters: {'nr_conv_blocks': 3, 'out_channels_0': 101, 'out_channels_1': 95, 'out_channels_2': 96, 'dropout_rate_0': 0.22671143900235458, 'dropout_rate_1': 0.5587086745243445, 'dropout_rate_2': 0.22921813054333795, 'nr_fc_layers': 3, 'fc_size_0': 538, 'fc_size_1': 1260, 'fc_size_2': 1516, 'dropout_rate_fc_0': 0.21977743274527745, 'dropout_rate_fc_1': 0.15772391388713344, 'dropout_rate_fc_2': 0.30881145847801167, 'optimizer': 'Adam', 'lr': 0.0001722435773054769, 'batch_size': 32}. Best is trial 3 with value: 0.9467101685698749.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 26               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 117, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.27714957979200844, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(117, 67, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.44887615314845464, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(67, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.29344333868444417, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(80, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.004451562796780885, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=4224, out_features=246, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.41479421061537297, inplace=False)\n",
            "    (3): Linear(in_features=246, out_features=451, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.07798541652435209, inplace=False)\n",
            "    (6): Linear(in_features=451, out_features=569, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.5740778049505196, inplace=False)\n",
            "    (9): Linear(in_features=569, out_features=2048, bias=True)\n",
            "    (10): ReLU()\n",
            "    (11): Dropout(p=0.6590121992955961, inplace=False)\n",
            "    (12): Linear(in_features=2048, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 117, 128, 128]           1,170\n",
            "              ReLU-2        [-1, 117, 128, 128]               0\n",
            "           Dropout-3        [-1, 117, 128, 128]               0\n",
            "         MaxPool2d-4          [-1, 117, 64, 64]               0\n",
            "         ConvBlock-5          [-1, 117, 64, 64]               0\n",
            "            Conv2d-6           [-1, 67, 64, 64]          70,618\n",
            "              ReLU-7           [-1, 67, 64, 64]               0\n",
            "           Dropout-8           [-1, 67, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 67, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 67, 32, 32]               0\n",
            "           Conv2d-11           [-1, 80, 32, 32]          48,320\n",
            "             ReLU-12           [-1, 80, 32, 32]               0\n",
            "          Dropout-13           [-1, 80, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 80, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 80, 16, 16]               0\n",
            "           Conv2d-16           [-1, 66, 16, 16]          47,586\n",
            "             ReLU-17           [-1, 66, 16, 16]               0\n",
            "          Dropout-18           [-1, 66, 16, 16]               0\n",
            "        MaxPool2d-19             [-1, 66, 8, 8]               0\n",
            "        ConvBlock-20             [-1, 66, 8, 8]               0\n",
            "           Linear-21                  [-1, 246]       1,039,350\n",
            "             ReLU-22                  [-1, 246]               0\n",
            "          Dropout-23                  [-1, 246]               0\n",
            "           Linear-24                  [-1, 451]         111,397\n",
            "             ReLU-25                  [-1, 451]               0\n",
            "          Dropout-26                  [-1, 451]               0\n",
            "           Linear-27                  [-1, 569]         257,188\n",
            "             ReLU-28                  [-1, 569]               0\n",
            "          Dropout-29                  [-1, 569]               0\n",
            "           Linear-30                 [-1, 2048]       1,167,360\n",
            "             ReLU-31                 [-1, 2048]               0\n",
            "          Dropout-32                 [-1, 2048]               0\n",
            "           Linear-33                   [-1, 28]          57,372\n",
            "================================================================\n",
            "Total params: 2,800,361\n",
            "Trainable params: 2,800,361\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 61.23\n",
            "Params size (MB): 10.68\n",
            "Estimated Total Size (MB): 71.98\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.332297  [  128/ 8582]\n",
            "Training loss: 3.109471  [ 6528/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:49:40,693] Trial 26 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 13.0%, Val - loss: 3.072116;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 27               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.1971589549780704, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(128, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5287576326378286, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(52, 107, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.39831176500484694, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(107, 91, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.2427216219529889, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (4): ConvBlock(\n",
            "      (conv): Conv2d(91, 79, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.2726060653070217, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=1264, out_features=451, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.3229213968553174, inplace=False)\n",
            "    (3): Linear(in_features=451, out_features=1604, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.06399963509986054, inplace=False)\n",
            "    (6): Linear(in_features=1604, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 128, 128, 128]           1,280\n",
            "              ReLU-2        [-1, 128, 128, 128]               0\n",
            "           Dropout-3        [-1, 128, 128, 128]               0\n",
            "         MaxPool2d-4          [-1, 128, 64, 64]               0\n",
            "         ConvBlock-5          [-1, 128, 64, 64]               0\n",
            "            Conv2d-6           [-1, 52, 64, 64]          59,956\n",
            "              ReLU-7           [-1, 52, 64, 64]               0\n",
            "           Dropout-8           [-1, 52, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 52, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 52, 32, 32]               0\n",
            "           Conv2d-11          [-1, 107, 32, 32]          50,183\n",
            "             ReLU-12          [-1, 107, 32, 32]               0\n",
            "          Dropout-13          [-1, 107, 32, 32]               0\n",
            "        MaxPool2d-14          [-1, 107, 16, 16]               0\n",
            "        ConvBlock-15          [-1, 107, 16, 16]               0\n",
            "           Conv2d-16           [-1, 91, 16, 16]          87,724\n",
            "             ReLU-17           [-1, 91, 16, 16]               0\n",
            "          Dropout-18           [-1, 91, 16, 16]               0\n",
            "        MaxPool2d-19             [-1, 91, 8, 8]               0\n",
            "        ConvBlock-20             [-1, 91, 8, 8]               0\n",
            "           Conv2d-21             [-1, 79, 8, 8]          64,780\n",
            "             ReLU-22             [-1, 79, 8, 8]               0\n",
            "          Dropout-23             [-1, 79, 8, 8]               0\n",
            "        MaxPool2d-24             [-1, 79, 4, 4]               0\n",
            "        ConvBlock-25             [-1, 79, 4, 4]               0\n",
            "           Linear-26                  [-1, 451]         570,515\n",
            "             ReLU-27                  [-1, 451]               0\n",
            "          Dropout-28                  [-1, 451]               0\n",
            "           Linear-29                 [-1, 1604]         725,008\n",
            "             ReLU-30                 [-1, 1604]               0\n",
            "          Dropout-31                 [-1, 1604]               0\n",
            "           Linear-32                   [-1, 28]          44,940\n",
            "================================================================\n",
            "Total params: 1,604,386\n",
            "Trainable params: 1,604,386\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 65.42\n",
            "Params size (MB): 6.12\n",
            "Estimated Total Size (MB): 71.60\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.330854  [  128/ 8582]\n",
            "Training loss: 3.537471  [ 6528/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:50:09,830] Trial 27 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 4.0%, Val - loss: 3.303128;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 28               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 82, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.14020092610084164, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(82, 107, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.6198967320298914, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(107, 65, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.33083359806667734, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (3): ConvBlock(\n",
            "      (conv): Conv2d(65, 115, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.10781344968616592, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=7360, out_features=648, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.25965797176820105, inplace=False)\n",
            "    (3): Linear(in_features=648, out_features=1159, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.13065490511604072, inplace=False)\n",
            "    (6): Linear(in_features=1159, out_features=1747, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.38707753279234747, inplace=False)\n",
            "    (9): Linear(in_features=1747, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 82, 128, 128]             820\n",
            "              ReLU-2         [-1, 82, 128, 128]               0\n",
            "           Dropout-3         [-1, 82, 128, 128]               0\n",
            "         MaxPool2d-4           [-1, 82, 64, 64]               0\n",
            "         ConvBlock-5           [-1, 82, 64, 64]               0\n",
            "            Conv2d-6          [-1, 107, 64, 64]          79,073\n",
            "              ReLU-7          [-1, 107, 64, 64]               0\n",
            "           Dropout-8          [-1, 107, 64, 64]               0\n",
            "         MaxPool2d-9          [-1, 107, 32, 32]               0\n",
            "        ConvBlock-10          [-1, 107, 32, 32]               0\n",
            "           Conv2d-11           [-1, 65, 32, 32]          62,660\n",
            "             ReLU-12           [-1, 65, 32, 32]               0\n",
            "          Dropout-13           [-1, 65, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 65, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 65, 16, 16]               0\n",
            "           Conv2d-16          [-1, 115, 16, 16]          67,390\n",
            "             ReLU-17          [-1, 115, 16, 16]               0\n",
            "          Dropout-18          [-1, 115, 16, 16]               0\n",
            "        MaxPool2d-19            [-1, 115, 8, 8]               0\n",
            "        ConvBlock-20            [-1, 115, 8, 8]               0\n",
            "           Linear-21                  [-1, 648]       4,769,928\n",
            "             ReLU-22                  [-1, 648]               0\n",
            "          Dropout-23                  [-1, 648]               0\n",
            "           Linear-24                 [-1, 1159]         752,191\n",
            "             ReLU-25                 [-1, 1159]               0\n",
            "          Dropout-26                 [-1, 1159]               0\n",
            "           Linear-27                 [-1, 1747]       2,026,520\n",
            "             ReLU-28                 [-1, 1747]               0\n",
            "          Dropout-29                 [-1, 1747]               0\n",
            "           Linear-30                   [-1, 28]          48,944\n",
            "================================================================\n",
            "Total params: 7,807,526\n",
            "Trainable params: 7,807,526\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 50.22\n",
            "Params size (MB): 29.78\n",
            "Estimated Total Size (MB): 80.07\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.331960  [   32/ 8582]\n",
            "Training loss: 111.306024  [ 1632/ 8582]\n",
            "Training loss: 2.964108  [ 3232/ 8582]\n",
            "Training loss: 2.922856  [ 4832/ 8582]\n",
            "Training loss: 2.821228  [ 6432/ 8582]\n",
            "Training loss: 2.772382  [ 8032/ 8582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:50:39,305] Trial 28 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: \n",
            " Val - Accuracy: 15.2%, Val - loss: 2.861873;\n",
            "\n",
            "\n",
            "\n",
            "#############################################\n",
            "#               Starting trial - 29               #\n",
            "#############################################\n",
            "\n",
            "Model\n",
            "FacesModel(\n",
            "  (conv_seq): Sequential(\n",
            "    (0): ConvBlock(\n",
            "      (conv): Conv2d(1, 106, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.07599516013958371, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): ConvBlock(\n",
            "      (conv): Conv2d(106, 83, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.5153222309434148, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): ConvBlock(\n",
            "      (conv): Conv2d(83, 93, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (activation): ReLU()\n",
            "      (dropout): Dropout(p=0.47234206670085344, inplace=False)\n",
            "      (max_pool2d): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (fc_seq): Sequential(\n",
            "    (0): Linear(in_features=23808, out_features=934, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.16441018257827333, inplace=False)\n",
            "    (3): Linear(in_features=934, out_features=1457, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.2664067764434715, inplace=False)\n",
            "    (6): Linear(in_features=1457, out_features=1300, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.4516856221272482, inplace=False)\n",
            "    (9): Linear(in_features=1300, out_features=28, bias=True)\n",
            "  )\n",
            ")\n",
            "Model Summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 106, 128, 128]           1,060\n",
            "              ReLU-2        [-1, 106, 128, 128]               0\n",
            "           Dropout-3        [-1, 106, 128, 128]               0\n",
            "         MaxPool2d-4          [-1, 106, 64, 64]               0\n",
            "         ConvBlock-5          [-1, 106, 64, 64]               0\n",
            "            Conv2d-6           [-1, 83, 64, 64]          79,265\n",
            "              ReLU-7           [-1, 83, 64, 64]               0\n",
            "           Dropout-8           [-1, 83, 64, 64]               0\n",
            "         MaxPool2d-9           [-1, 83, 32, 32]               0\n",
            "        ConvBlock-10           [-1, 83, 32, 32]               0\n",
            "           Conv2d-11           [-1, 93, 32, 32]          69,564\n",
            "             ReLU-12           [-1, 93, 32, 32]               0\n",
            "          Dropout-13           [-1, 93, 32, 32]               0\n",
            "        MaxPool2d-14           [-1, 93, 16, 16]               0\n",
            "        ConvBlock-15           [-1, 93, 16, 16]               0\n",
            "           Linear-16                  [-1, 934]      22,237,606\n",
            "             ReLU-17                  [-1, 934]               0\n",
            "          Dropout-18                  [-1, 934]               0\n",
            "           Linear-19                 [-1, 1457]       1,362,295\n",
            "             ReLU-20                 [-1, 1457]               0\n",
            "          Dropout-21                 [-1, 1457]               0\n",
            "           Linear-22                 [-1, 1300]       1,895,400\n",
            "             ReLU-23                 [-1, 1300]               0\n",
            "          Dropout-24                 [-1, 1300]               0\n",
            "           Linear-25                   [-1, 28]          36,428\n",
            "================================================================\n",
            "Total params: 25,681,618\n",
            "Trainable params: 25,681,618\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 58.08\n",
            "Params size (MB): 97.97\n",
            "Estimated Total Size (MB): 156.11\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "Total dataset size: 12260; split in train=8582, validation=1839, test=1839\n",
            "\n",
            "------------------------------------------ Epoch=1 ------------------------------------------\n",
            "\n",
            "Training loss: 3.331345  [  128/ 8582]\n",
            "Training loss: 3.332614  [ 6528/ 8582]\n",
            "Validation: \n",
            " Val - Accuracy: 3.6%, Val - loss: 3.332815;\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-08-16 10:51:09,902] Trial 29 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Study statistics: \n",
            "  Number of finished trials:  30\n",
            "  Number of pruned trials:  21\n",
            "  Number of complete trials:  9\n",
            "Best trial:\n",
            "  Value:  0.9467101685698749\n",
            "  Params: \n",
            "    nr_conv_blocks: 4\n",
            "    out_channels_0: 90\n",
            "    out_channels_1: 61\n",
            "    out_channels_2: 81\n",
            "    out_channels_3: 94\n",
            "    dropout_rate_0: 0.2600750701045177\n",
            "    dropout_rate_1: 0.5511384039955239\n",
            "    dropout_rate_2: 0.2768973776544742\n",
            "    dropout_rate_3: 0.12301003766717822\n",
            "    nr_fc_layers: 1\n",
            "    fc_size_0: 259\n",
            "    dropout_rate_fc_0: 0.5886458883013169\n",
            "    optimizer: SGD\n",
            "    lr: 0.021218488681107886\n",
            "    batch_size: 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"082dcb28-f778-4c29-bf10-9818a0e4ca54\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"082dcb28-f778-4c29-bf10-9818a0e4ca54\")) {                    Plotly.newPlot(                        \"082dcb28-f778-4c29-bf10-9818a0e4ca54\",                        [{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial0\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.16367591082109842,0.27351821642196844,0.4622077215878195,0.7030995106035889,0.8167482327351822,0.8080478520935291,0.8733007069059271,0.8738444806960305,0.8950516585100599,0.9119086460032626,0.9363784665579119,0.9222403480152257,0.9314845024469821,0.9265905383360522,0.9271343121261555,0.9303969548667754,0.9314845024469821,0.9396411092985318,0.9249592169657422,0.9314845024469821,0.9342033713974986,0.9336595976073954,0.9358346927678086,0.9358346927678086,0.9369222403480152,0.9380097879282219,0.9293094072865687,0.9423599782490484,0.9499728113104948,0.9276780859162589],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial1\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.048395867319195214,0.03806416530723219,0.03806416530723219,0.048395867319195214,0.03806416530723219,0.03806416530723219,0.03806416530723219,0.03806416530723219,0.03806416530723219,0.03806416530723219,0.03806416530723219,0.03806416530723219,0.03806416530723219,0.03806416530723219,0.03806416530723219,0.04676454594888527,0.03806416530723219,0.03806416530723219,0.03860793909733551,0.03806416530723219,0.02773246329526917,0.037520391517128875,0.037520391517128875,0.048395867319195214,0.06199021207177814,0.048395867319195214,0.05872756933115824,0.037520391517128875,0.037520391517128875,0.052202283849918436],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial2\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.13594344752582926,0.18325176726481784,0.24850462207721588,0.3094072865687874,0.4964654703643284,0.7210440456769984,0.5905383360522023,0.7754214246873301,0.8102229472539424,0.6867862969004894,0.8395867319195215,0.768352365415987,0.8330614464382817,0.8401305057096248,0.8444806960304513,0.831973898858075,0.8553561718325177,0.8526373028820011,0.8618814573137574,0.8042414355628059,0.8520935290918977,0.8471995649809679,0.8771071234366503,0.8520935290918977,0.866231647634584,0.8760195758564437,0.8607939097335509,0.8754758020663403,0.8417618270799347,0.8466557911908646],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial3\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.1348558999456226,0.41163675910821096,0.4480696030451332,0.610657966286025,0.8064165307232192,0.7264817835780315,0.6405655247417075,0.8684067427949973,0.88689505165851,0.9146275149537793,0.9086460032626428,0.9178901576943991,0.9249592169657422,0.9249592169657422,0.8705818379554106,0.9390973355084284,0.9358346927678086,0.9374660141381186,0.9342033713974986,0.9407286568787384,0.9276780859162589,0.945078847199565,0.9423599782490484,0.9445350734094616,0.9265905383360522,0.9363784665579119,0.9472539423599783,0.9472539423599783,0.9412724306688418,0.9467101685698749],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial4\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.24361065796628603,0.44426318651441,0.5704187058183795,0.6960304513322458,0.7699836867862969,0.8167482327351822,0.8314301250679718,0.843936922240348,0.8531810766721044,0.8488308863512779,0.8651441000543774,0.867862969004894,0.8776508972267537,0.8836324089178902,0.8923327895595432,0.8928765633496466,0.9048395867319196,0.9042958129418162,0.9053833605220228,0.9135399673735726,0.9081022294725394,0.9162588363240892,0.9157150625339858,0.9173463839042958,0.9200652528548124,0.9168026101141925,0.9233278955954323,0.9265905383360522,0.9184339314845025,0.9233278955954323],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial5\",\"x\":[0],\"y\":[0.057096247960848286],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial6\",\"x\":[0,1,2,3],\"y\":[0.22784121805328983,0.4176182707993475,0.4937466014138119,0.5889070146818923],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial7\",\"x\":[0,1,2],\"y\":[0.15171288743882544,0.279499728113105,0.28602501359434473],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial8\",\"x\":[0],\"y\":[0.04023926046764546],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial9\",\"x\":[0],\"y\":[0.027188689505165852],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial10\",\"x\":[0],\"y\":[0.12561174551386622],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial11\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.18107667210440456,0.3333333333333333,0.5562805872756933,0.7623708537248505,0.8205546492659054,0.8646003262642741,0.876563349646547,0.8988580750407831,0.9129961935834693,0.9059271343121261,0.9064709081022295,0.9271343121261555,0.9129961935834693,0.921152800435019,0.9238716693855357,0.9113648722131593,0.9113648722131593,0.912452419793366,0.9303969548667754,0.9200652528548124,0.9222403480152257,0.919521479064709,0.922784121805329,0.9249592169657422,0.9216965742251223,0.9075584556824361,0.919521479064709,0.9347471451876019,0.9385535617183252,0.9293094072865687],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial12\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.2207721587819467,0.41707449700924415,0.6867862969004894,0.8129418162044589,0.8575312669929309,0.8646003262642741,0.8852637302882002,0.9004893964110929,0.8912452419793366,0.8945078847199565,0.9048395867319196,0.8863512778684067,0.9129961935834693,0.9309407286568787,0.921152800435019,0.9157150625339858,0.9276780859162589,0.9320282762370854,0.9206090266449157,0.9380097879282219,0.9407286568787384,0.9276780859162589,0.9352909189777052,0.9374660141381186,0.9222403480152257,0.9238716693855357,0.9407286568787384,0.9401848830886351,0.9222403480152257,0.9358346927678086],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial13\",\"x\":[0],\"y\":[0.03806416530723219],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial14\",\"x\":[0,1,2,3,4,5,6],\"y\":[0.29853181076672103,0.5861881457313758,0.609026644915715,0.7509516041326808,0.8216421968461121,0.7678085916258837,0.8118542686242524],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial15\",\"x\":[0],\"y\":[0.057640021750951606],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial16\",\"x\":[0],\"y\":[0.04241435562805873],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial17\",\"x\":[0],\"y\":[0.11854268624252311],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial18\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13],\"y\":[0.39206090266449156,0.8085916258836324,0.878194671016857,0.89015769439913,0.8896139206090267,0.8961392060902664,0.9091897770527461,0.9146275149537793,0.9021207177814029,0.9070146818923328,0.8955954323001631,0.8825448613376835,0.8972267536704731,0.9102773246329527],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial19\",\"x\":[0],\"y\":[0.03643284393692224],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial20\",\"x\":[0],\"y\":[0.03371397498640565],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial21\",\"x\":[0,1],\"y\":[0.19738988580750408,0.3164763458401305],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial22\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.1854268624252311,0.3610657966286025,0.6922240348015226,0.8064165307232192,0.8395867319195215,0.8656878738444806,0.8749320282762371,0.8771071234366503,0.8945078847199565,0.9026644915715063,0.922784121805329,0.9113648722131593,0.9173463839042958,0.9189777052746058,0.919521479064709,0.910821098423056,0.9184339314845025,0.922784121805329,0.8950516585100599,0.9216965742251223,0.9314845024469821,0.9314845024469821,0.9216965742251223,0.9249592169657422,0.9358346927678086,0.9309407286568787,0.9347471451876019,0.924415443175639,0.9314845024469821,0.9287656334964655],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial23\",\"x\":[0],\"y\":[0.1653072321914084],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial24\",\"x\":[0,1,2,3,4,5,6,7,8,9,10],\"y\":[0.2207721587819467,0.6655791190864601,0.7939097335508428,0.8379554105492115,0.855899945622621,0.8830886351277868,0.878194671016857,0.8841761827079935,0.9042958129418162,0.8890701468189234,0.8711256117455138],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial25\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],\"y\":[0.5535617183251768,0.8689505165851006,0.9026644915715063,0.9336595976073954,0.9157150625339858,0.9282218597063622,0.924415443175639,0.9282218597063622,0.9282218597063622,0.9249592169657422,0.9314845024469821,0.9358346927678086,0.9200652528548124,0.9352909189777052,0.922784121805329,0.9358346927678086,0.933115823817292,0.9342033713974986,0.9206090266449157,0.9271343121261555,0.9418162044589451,0.9385535617183252,0.945078847199565,0.9461663947797716,0.9363784665579119,0.9456226209896683,0.9488852637302883,0.9418162044589451,0.9396411092985318,0.9265905383360522],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial26\",\"x\":[0],\"y\":[0.12996193583469276],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial27\",\"x\":[0],\"y\":[0.03969548667754214],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial28\",\"x\":[0],\"y\":[0.15225666122892875],\"type\":\"scatter\"},{\"marker\":{\"maxdisplayed\":10},\"mode\":\"lines+markers\",\"name\":\"Trial29\",\"x\":[0],\"y\":[0.03588907014681892],\"type\":\"scatter\"}],                        {\"showlegend\":false,\"title\":{\"text\":\"Intermediate Values Plot\"},\"xaxis\":{\"title\":{\"text\":\"Step\"}},\"yaxis\":{\"title\":{\"text\":\"Intermediate Value\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('082dcb28-f778-4c29-bf10-9818a0e4ca54');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "execute()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "best_trials = [trial for trial in study.trials if trial.value>0.9]\n",
        "# Collect hyperparameter values from the best trials\n",
        "learning_rates = [trial.params.get(\"lr\") for trial in best_trials]\n",
        "fc_layers = [trial.params.get(\"nr_fc_layers\") for trial in best_trials]\n",
        "batch_sizes = [trial.params.get(\"batch_size\") for trial in best_trials]\n",
        "conv_layers = [trial.params.get(\"nr_conv_blocks\") for trial in best_trials]\n",
        "optimizers = [trial.params.get(\"optimizer\") for trial in best_trials]\n",
        "\n",
        "# Count occurrences of each hyperparameter value\n",
        "learning_rates_counts = Counter(learning_rates)\n",
        "fc_layers_counts = Counter(fc_layers)\n",
        "batch_size_counts = Counter(batch_sizes)\n",
        "conv_layer_counts = Counter(conv_layers)\n",
        "optimizer_counts = Counter(optimizers)\n",
        "\n",
        "# Plot histograms\n",
        "def plot_histogram(counter, title, xlabel):\n",
        "    plt.bar(counter.keys(), counter.values())\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "bins = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
        "bin_labels = [\"1e-5\", \"1e-4\", \"1e-3\", \"1e-2\", \"1e-1\"]\n",
        "categorized_learning_rates = np.digitize(list(learning_rates_counts.keys()), bins)\n",
        "categorized_counts = Counter(categorized_learning_rates)\n",
        "\n",
        "plt.bar(bin_labels, [categorized_counts[i] for i in range(1, len(bins) + 1)])\n",
        "plt.xlabel(\"Learning Rate Ranges\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Learning Rate Histogram\")\n",
        "plt.show()\n",
        "\n",
        "plot_histogram(fc_layers_counts, \"Fully Connected Layer Histogram\", \"Number of Fully Connected Layers\")\n",
        "plot_histogram(batch_size_counts, \"Batch Size Histogram\", \"Batch Size\")\n",
        "plot_histogram(conv_layer_counts, \"Convolutional Layer Histogram\", \"Number of Convolutional Layers\")\n",
        "plot_histogram(optimizer_counts, \"Optimizer Histogram\", \"Optimizer Choice\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g0VcVIPO0Al3",
        "outputId": "c40e100a-a124-4e43-a393-3ead353fc973"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusions after this optimization session:\n",
        "- we can safely assum that the Adam optimizer is the best one for this scenario as it is used in the majority of the succesful (trial.value >0.9) trials\n",
        "- best batch sizes are 32 and 128.\n",
        "- learning rate can be limited to 1.e-5 and 1.e-3\n",
        "- number of fc layers and of conv layers is not yet clear, but will be decided in next tests."
      ],
      "metadata": {
        "id": "vOz9wJ-R0Cj3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h6xIJE7o0boC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "NSkt9_hR0BVv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}